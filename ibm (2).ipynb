{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "cell_execution_strategy": "setup"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qeS1bQiz4qHH"
      },
      "outputs": [],
      "source": [
        "# Install required libraries\n",
        "!pip install pandas numpy scikit-learn tensorflow aif360 fairlearn matplotlib seaborn hyperopt\n",
        "\n",
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, roc_auc_score,\n",
        "    f1_score, average_precision_score\n",
        ")\n",
        "\n",
        "# AIF360 Libraries\n",
        "from aif360.datasets import StandardDataset\n",
        "from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric\n",
        "from aif360.algorithms.preprocessing import Reweighing\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "import gc\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from joblib import dump\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import warnings\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4FtVUJPviRTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MZtCMr9PiTBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load the dataset\n",
        "# Replace 'acs_income.csv' with the actual file path or URL\n",
        "data = pd.read_csv(\"acsincome_raw_2018.csv\")# Display the first 5 rows of the dataset\n",
        "print(\"First 5 rows of the dataset:\")\n",
        "print(data.head())\n",
        "\n",
        "#%% [code]\n",
        "# Import necessary libraries\n",
        "import os\n",
        "import gc\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from joblib import dump\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import warnings\n",
        "\n",
        "# Suppress warnings for cleaner output (optional)\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Load the dataset (modify the file path if needed)\n",
        "data = pd.read_csv(\"acsincome_raw_2018.csv\")\n",
        "\n",
        "# Display first 5 rows and basic info\n",
        "print(\"First 5 rows of the raw dataset:\")\n",
        "print(data.head())\n",
        "\n",
        "print(\"\\nDataset Info:\")\n",
        "print(data.info())\n",
        "\n",
        "print(\"\\nMissing Values Before Cleaning:\")\n",
        "print(data.isnull().sum())\n"
      ],
      "metadata": {
        "id": "HWULeMSa5WTQ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%% [code]\n",
        "# Remove duplicate rows\n",
        "data = data.drop_duplicates()\n",
        "print(\"\\nDuplicates removed. New shape:\", data.shape)\n",
        "\n",
        "# Drop constant columns (if any)\n",
        "nunique = data.nunique()\n",
        "constant_cols = nunique[nunique <= 1].index.tolist()\n",
        "if constant_cols:\n",
        "    data.drop(constant_cols, axis=1, inplace=True)\n",
        "    print(f\"\\nDropped constant columns: {constant_cols}\")\n",
        "\n",
        "# Optionally: Cap extreme outliers for numerical columns using 1st and 99th percentiles\n",
        "def cap_outliers(df, col):\n",
        "    lower = df[col].quantile(0.01)\n",
        "    upper = df[col].quantile(0.99)\n",
        "    df[col] = np.clip(df[col], lower, upper)\n",
        "\n",
        "for col in data.select_dtypes(include=[np.number]).columns:\n",
        "    cap_outliers(data, col)\n",
        "\n",
        "# Impute numerical columns with median\n",
        "numerical_cols = data.select_dtypes(include=['float64', 'int64']).columns\n",
        "data[numerical_cols] = data[numerical_cols].fillna(data[numerical_cols].median())\n",
        "\n",
        "# Impute categorical columns with 'Unknown'\n",
        "categorical_cols = data.select_dtypes(include=['object']).columns\n",
        "data[categorical_cols] = data[categorical_cols].fillna('Unknown')\n",
        "\n",
        "print(\"\\nMissing Values After Imputation:\")\n",
        "print(data.isnull().sum())\n"
      ],
      "metadata": {
        "id": "tB8_zU325bHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%% [code]\n",
        "# Convert the continuous income column 'PINCP' to a binary target.\n",
        "# Income > $50,000 is considered favorable (1), else unfavorable (0).\n",
        "data['Income_Binary'] = data['PINCP'].apply(lambda x: 1 if x > 50000 else 0)\n",
        "\n",
        "# Drop the original income column\n",
        "data.drop('PINCP', axis=1, inplace=True)\n",
        "\n",
        "print(\"\\nDistribution of Income_Binary:\")\n",
        "print(data['Income_Binary'].value_counts())\n",
        "\n",
        "# Split Data into Features (X) and Target (y)\n",
        "X = data.drop('Income_Binary', axis=1)\n",
        "y = data['Income_Binary']\n",
        "\n",
        "# Encode categorical variables using one-hot encoding (drop_first to avoid dummy variable trap)\n",
        "X = pd.get_dummies(X, drop_first=True)\n",
        "print(\"\\nShape after encoding:\", X.shape)\n"
      ],
      "metadata": {
        "id": "uWvNKERY5iWO",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%% [code]\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "print(\"\\nTraining set shape:\", X_train.shape)\n",
        "print(\"Testing set shape:\", X_test.shape)\n",
        "\n",
        "# Train a Baseline RandomForest Classifier\n",
        "baseline_model = RandomForestClassifier(random_state=42)\n",
        "baseline_model.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "rDyi13WSeAiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%% [code]\n",
        "# Evaluate the Baseline Model\n",
        "y_pred = baseline_model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, y_pred)\n",
        "\n",
        "print(\"\\nBaseline Model Evaluation:\")\n",
        "print(f\"Accuracy:  {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall:    {recall:.4f}\")\n",
        "print(f\"ROC-AUC:   {roc_auc:.4f}\")\n",
        "\n",
        "# Clear memory if needed\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "OHZLz8vOllmw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ciVXonEeHAOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Define the directory path to save the cleaned data, model, and metrics.\n",
        "# Modify this path if using Google Colab. For example, you can use a mounted Google Drive path.\n",
        "local_dir = r\"C:\\Users\\rohan\\Downloads\\IBM COLLAB\"\n",
        "if not os.path.exists(local_dir):\n",
        "    os.makedirs(local_dir)\n",
        "    print(f\"\\nDirectory created: {local_dir}\")\n",
        "\n",
        "# Save the preprocessed dataset to CSV\n",
        "cleaned_data_path = os.path.join(local_dir, \"preprocessed_acs_income.csv\")\n",
        "data.to_csv(cleaned_data_path, index=False)\n",
        "print(f\"\\nPreprocessed dataset saved to: {cleaned_data_path}\")\n",
        "\n",
        "# Save the baseline model using joblib\n",
        "model_path = os.path.join(local_dir, \"baseline_model.joblib\")\n",
        "dump(baseline_model, model_path)\n",
        "print(f\"Baseline model saved to: {model_path}\")\n",
        "\n",
        "# Save evaluation metrics to a JSON file\n",
        "metrics = {\n",
        "    \"Accuracy\": accuracy,\n",
        "    \"Precision\": precision,\n",
        "    \"Recall\": recall,\n",
        "    \"ROC-AUC\": roc_auc\n",
        "}\n",
        "metrics_path = os.path.join(local_dir, \"baseline_metrics.json\")\n",
        "with open(metrics_path, \"w\") as f:\n",
        "    json.dump(metrics, f)\n",
        "print(f\"Baseline metrics saved to: {metrics_path}\")\n"
      ],
      "metadata": {
        "id": "FyvJpC0Q5lXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "\n",
        "# Load the data variables\n",
        "import pickle\n",
        "with open(variables_path, 'rb') as f:\n",
        "    data_variables = pickle.load(f)\n",
        "\n",
        "# Extract y_test (true labels)\n",
        "y_true = data_variables[\"y_test\"]\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_true, final_predictions)\n",
        "precision = precision_score(y_true, final_predictions)\n",
        "recall = recall_score(y_true, final_predictions)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)"
      ],
      "metadata": {
        "id": "w6FWtFPi5mVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%% [code]\n",
        "import pickle\n",
        "\n",
        "# Define the file path where the variables were saved\n",
        "variables_path = os.path.join(local_dir, \"data_variables.pkl\")\n",
        "\n",
        "# Load the variables\n",
        "with open(variables_path, \"rb\") as f:\n",
        "    loaded_variables = pickle.load(f)\n",
        "\n",
        "# Assign the variables to your workspace\n",
        "data = loaded_variables[\"data\"]\n",
        "X = loaded_variables[\"X\"]\n",
        "y = loaded_variables[\"y\"]\n",
        "X_train = loaded_variables[\"X_train\"]\n",
        "X_test = loaded_variables[\"X_test\"]\n",
        "y_train = loaded_variables[\"y_train\"]\n",
        "y_test = loaded_variables[\"y_test\"]\n",
        "\n",
        "print(\"All key variables have been successfully loaded.\")\n",
        "print(\"Shapes:\")\n",
        "print(f\"data: {data.shape}\")\n",
        "print(f\"X: {X.shape}\")\n",
        "print(f\"X_train: {X_train.shape}\")\n",
        "print(f\"X_test: {X_test.shape}\")\n"
      ],
      "metadata": {
        "id": "pq9ji_eLjaDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create X_test_df by copying X_test and adding the target column from y_test\n",
        "X_test_df = X_test.copy()\n",
        "X_test_df[\"Income_Binary\"] = y_test.values\n",
        "\n",
        "# Now, use X_test_df to create your AIF360 test dataset:\n",
        "from aif360.datasets import BinaryLabelDataset\n",
        "\n",
        "test_dataset = BinaryLabelDataset(\n",
        "    df=X_test_df,\n",
        "    label_names=[\"Income_Binary\"],\n",
        "    protected_attribute_names=[],  # Specify sensitive attributes if needed\n",
        "    favorable_label=1,\n",
        "    unfavorable_label=0\n",
        ")\n",
        "\n",
        "print(\"X_test_df created and test_dataset built successfully!\")\n"
      ],
      "metadata": {
        "id": "pYRNvDZD36ba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%% [code]\n",
        "from aif360.datasets import BinaryLabelDataset\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "# Create the full dataset (if needed)\n",
        "dataset_full = BinaryLabelDataset(\n",
        "    df=data,\n",
        "    label_names=[\"Income_Binary\"],\n",
        "    protected_attribute_names=[],  # Fill in if needed\n",
        "    favorable_label=1,\n",
        "    unfavorable_label=0\n",
        ")\n",
        "\n",
        "# For demonstration, create datasets for training and testing.\n",
        "# Merge X_train with y_train for the training DataFrame.\n",
        "train_df = X_train.copy()\n",
        "train_df[\"Income_Binary\"] = y_train.values\n",
        "\n",
        "# Create the test DataFrame by merging X_test with y_test.\n",
        "# Here, we define X_test_df\n",
        "X_test_df = X_test.copy()\n",
        "X_test_df[\"Income_Binary\"] = y_test.values\n",
        "\n",
        "# Create BinaryLabelDataset objects using the respective DataFrames.\n",
        "train_dataset = BinaryLabelDataset(\n",
        "    df=train_df,\n",
        "    label_names=[\"Income_Binary\"],\n",
        "    protected_attribute_names=[],  # Specify sensitive attribute names if needed\n",
        "    favorable_label=1,\n",
        "    unfavorable_label=0\n",
        ")\n",
        "test_dataset = BinaryLabelDataset(\n",
        "    df=X_test_df,\n",
        "    label_names=[\"Income_Binary\"],\n",
        "    protected_attribute_names=[],  # Specify sensitive attribute names if needed\n",
        "    favorable_label=1,\n",
        "    unfavorable_label=0\n",
        ")\n",
        "\n",
        "# Save these objects using pickle.\n",
        "local_dir = r\"C:\\Users\\rohan\\Downloads\\IBM COLLAB\"  # Update directory as needed\n",
        "datasets_path = os.path.join(local_dir, \"classification_datasets.pkl\")\n",
        "with open(datasets_path, \"wb\") as f:\n",
        "    pickle.dump({\"train_dataset\": train_dataset, \"test_dataset\": test_dataset}, f)\n",
        "\n",
        "print(f\"Classification datasets saved to: {datasets_path}\")\n"
      ],
      "metadata": {
        "id": "rizW-Z2c5zi5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%% [code]\n",
        "import pickle\n",
        "\n",
        "datasets_path = os.path.join(local_dir, \"classification_datasets.pkl\")\n",
        "with open(datasets_path, \"rb\") as f:\n",
        "    loaded_datasets = pickle.load(f)\n",
        "\n",
        "train_dataset = loaded_datasets[\"train_dataset\"]\n",
        "test_dataset = loaded_datasets[\"test_dataset\"]\n",
        "\n",
        "print(\"Classification datasets have been successfully loaded.\")\n"
      ],
      "metadata": {
        "id": "8ra5E52Dj1hx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the preprocessed dataset\n",
        "data = pd.read_csv(r\"C:\\Users\\rohan\\Downloads\\IBM COLLAB\\preprocessed_acs_income.csv\")\n",
        "\n",
        "# Display the first few rows\n",
        "print(data.head())"
      ],
      "metadata": {
        "id": "0hEyXvvEJQeO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = pd.get_dummies(data.drop('Income_Binary', axis=1), drop_first=True)\n",
        "y = data['Income_Binary']"
      ],
      "metadata": {
        "id": "FpTEZdiJ1eQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from aif360.metrics import ClassificationMetric\n",
        "\n",
        "def compute_fairness_metrics(dataset, pred_dataset, privileged_groups, unprivileged_groups):\n",
        "    # Compute fairness metrics using AIF360's ClassificationMetric\n",
        "    metric = ClassificationMetric(\n",
        "        dataset,\n",
        "        pred_dataset,\n",
        "        privileged_groups=privileged_groups,\n",
        "        unprivileged_groups=unprivileged_groups\n",
        "    )\n",
        "    metrics = {\n",
        "        \"Disparate Impact\": metric.disparate_impact(),\n",
        "        \"Statistical Parity Difference\": metric.statistical_parity_difference(),\n",
        "        \"Equal Opportunity Difference\": metric.equal_opportunity_difference(),\n",
        "        \"Average Odds Difference\": metric.average_odds_difference(),\n",
        "        \"Theil Index\": metric.theil_index()\n",
        "    }\n",
        "    return metrics\n",
        "\n",
        "def display_fairness_metrics(metrics, group_name):\n",
        "    print(f\"\\nFairness Metrics for {group_name}:\")\n",
        "    for metric_name, value in metrics.items():\n",
        "        print(f\"  {metric_name}: {value:.4f}\")\n"
      ],
      "metadata": {
        "id": "XShmNLrN4--w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assume X and y are already defined from preprocessing.\n",
        "# Example:\n",
        "# X = pd.get_dummies(data.drop('Income_Binary', axis=1), drop_first=True)\n",
        "# y = data['Income_Binary']\n",
        "\n",
        "# Create and save a consistent train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Save the train-test split into a pickle file\n",
        "with open('data_variables.pkl', 'wb') as f:\n",
        "    pickle.dump({\n",
        "        \"X_train\": X_train,\n",
        "        \"X_test\": X_test,\n",
        "        \"y_train\": y_train,\n",
        "        \"y_test\": y_test\n",
        "    }, f)\n",
        "\n",
        "print(\"✅ Train and test split saved as 'data_variables.pkl'.\")\n"
      ],
      "metadata": {
        "id": "tkTx-labEWJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(\"Current Directory:\", os.getcwd())  # Check your working directory\n",
        "print(\"Files in Directory:\", os.listdir())  # List available files\n"
      ],
      "metadata": {
        "id": "aI6uLSozEf_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from aif360.datasets import BinaryLabelDataset\n",
        "\n",
        "# Convert datasets to BinaryLabelDataset format using the correct parameters.\n",
        "# Here, we assume that your binary target variable, \"Income_Binary\", has values 0 and 1,\n",
        "# where 1 is considered the favorable outcome.\n",
        "\n",
        "test_dataset_sex = BinaryLabelDataset(\n",
        "    df=X_test_df,\n",
        "    label_names=[\"Income_Binary\"],\n",
        "    protected_attribute_names=[\"SEX\"],\n",
        "    favorable_label=1,\n",
        "    unfavorable_label=0\n",
        ")\n",
        "\n",
        "test_dataset_race = BinaryLabelDataset(\n",
        "    df=X_test_df,\n",
        "    label_names=[\"Income_Binary\"],\n",
        "    protected_attribute_names=[\"RAC1P\"],\n",
        "    favorable_label=1,\n",
        "    unfavorable_label=0\n",
        ")\n",
        "\n",
        "test_dataset_age = BinaryLabelDataset(\n",
        "    df=X_test_df,\n",
        "    label_names=[\"Income_Binary\"],\n",
        "    protected_attribute_names=[\"AGEP\"],\n",
        "    favorable_label=1,\n",
        "    unfavorable_label=0\n",
        ")\n",
        "\n",
        "test_dataset_mar = BinaryLabelDataset(\n",
        "    df=X_test_df,\n",
        "    label_names=[\"Income_Binary\"],\n",
        "    protected_attribute_names=[\"MAR\"],\n",
        "    favorable_label=1,\n",
        "    unfavorable_label=0\n",
        ")\n"
      ],
      "metadata": {
        "id": "xgpK8BDU6Zta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure X_test_df contains only the feature columns (excluding the target column)\n",
        "X_test_features = X_test_df.drop(columns=[\"Income_Binary\"])\n",
        "\n",
        "# Generate predicted labels\n",
        "y_pred = baseline_model.predict(X_test_features)\n",
        "\n",
        "# Create predicted datasets\n",
        "test_pred_dataset_sex = test_dataset_sex.copy()\n",
        "test_pred_dataset_sex.labels = y_pred.reshape(-1, 1)\n",
        "\n",
        "test_pred_dataset_race = test_dataset_race.copy()\n",
        "test_pred_dataset_race.labels = y_pred.reshape(-1, 1)\n",
        "\n",
        "test_pred_dataset_age = test_dataset_age.copy()\n",
        "test_pred_dataset_age.labels = y_pred.reshape(-1, 1)\n",
        "\n",
        "test_pred_dataset_mar = test_dataset_mar.copy()\n",
        "test_pred_dataset_mar.labels = y_pred.reshape(-1, 1)\n",
        "\n",
        "# Define privileged and unprivileged groups\n",
        "privileged_groups_sex = [{\"SEX\": 1}]  # Privileged group (Male)\n",
        "unprivileged_groups_sex = [{\"SEX\": 2}]  # Unprivileged group (Female)\n",
        "\n",
        "privileged_groups_race = [{\"RAC1P\": 1}]  # Privileged group (White)\n",
        "unprivileged_groups_race = [{\"RAC1P\": i} for i in range(2, 10)]  # Unprivileged group (Non-White)\n",
        "\n",
        "median_age = X_test_df[\"AGEP\"].median()\n",
        "privileged_groups_age = [{\"AGEP\": i} for i in X_test_df[\"AGEP\"].unique() if i >= median_age]\n",
        "unprivileged_groups_age = [{\"AGEP\": i} for i in X_test_df[\"AGEP\"].unique() if i < median_age]\n",
        "\n",
        "privileged_groups_mar = [{\"MAR\": 1.0}]  # Privileged group (Married)\n",
        "unprivileged_groups_mar = [{\"MAR\": i} for i in X_test_df[\"MAR\"].unique() if i != 1.0]  # Unprivileged group (Unmarried)\n",
        "\n",
        "# Compute fairness metrics\n",
        "fairness_metrics_sex = compute_fairness_metrics(test_dataset_sex, test_pred_dataset_sex,\n",
        "                                                privileged_groups_sex, unprivileged_groups_sex)\n",
        "fairness_metrics_race = compute_fairness_metrics(test_dataset_race, test_pred_dataset_race,\n",
        "                                                privileged_groups_race, unprivileged_groups_race)\n",
        "fairness_metrics_age = compute_fairness_metrics(test_dataset_age, test_pred_dataset_age,\n",
        "                                                privileged_groups_age, unprivileged_groups_age)\n",
        "fairness_metrics_mar = compute_fairness_metrics(test_dataset_mar, test_pred_dataset_mar,\n",
        "                                                privileged_groups_mar, unprivileged_groups_mar)\n",
        "\n",
        "# Display the results\n",
        "display_fairness_metrics(fairness_metrics_sex, \"Gender (SEX)\")\n",
        "display_fairness_metrics(fairness_metrics_race, \"Race (RAC1P)\")\n",
        "display_fairness_metrics(fairness_metrics_age, \"Age (AGEP)\")\n",
        "display_fairness_metrics(fairness_metrics_mar, \"Marital Status (MAR)\")"
      ],
      "metadata": {
        "id": "_8zXGXu4-yzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Assuming your fairness metrics have been computed and stored in a dictionary:\n",
        "all_fairness_metrics = {\n",
        "    \"Gender (SEX)\": fairness_metrics_sex,\n",
        "    \"Race (RAC1P)\": fairness_metrics_race,\n",
        "    \"Age (AGEP)\": fairness_metrics_age,\n",
        "    \"Marital Status (MAR)\": fairness_metrics_mar\n",
        "}\n",
        "\n",
        "# Define the output file path (update as needed)\n",
        "output_path = r\"C:\\Users\\rohan\\Downloads\\IBM COLLAB\\all_fairness_metrics.json\"\n",
        "\n",
        "# Save the fairness metrics to the JSON file\n",
        "with open(output_path, \"w\") as f:\n",
        "    json.dump(all_fairness_metrics, f, indent=4)\n",
        "\n",
        "print(\"Fairness metrics have been saved to:\")\n",
        "print(output_path)\n"
      ],
      "metadata": {
        "id": "tASEaeBagl9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Define the input file path (must match the output path)\n",
        "input_path = r\"C:\\Users\\rohan\\Downloads\\IBM COLLAB\\all_fairness_metrics.json\"\n",
        "\n",
        "# Load the fairness metrics from the JSON file\n",
        "with open(input_path, \"r\") as f:\n",
        "    loaded_metrics = json.load(f)\n",
        "\n",
        "print(\"Loaded Fairness Metrics:\")\n",
        "for group, metrics in loaded_metrics.items():\n",
        "    print(f\"\\n{group}:\")\n",
        "    for metric, value in metrics.items():\n",
        "        print(f\"  {metric}: {value:.4f}\")\n"
      ],
      "metadata": {
        "id": "5YG-oYc-hJwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Unique values in SEX column:\", X_test_df[\"SEX\"].unique())"
      ],
      "metadata": {
        "id": "E0MLMNszP25G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Unique values in y_pred:\", np.unique(y_pred))\n"
      ],
      "metadata": {
        "id": "a4IYqXSC5sk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert 'SEX' to binary format (1 = Male, 0 = Female)\n",
        "X_test_df[\"SEX\"] = X_test_df[\"SEX\"].replace({2: 0})\n",
        "print(\"Updated unique values in SEX column:\", X_test_df[\"SEX\"].unique())\n"
      ],
      "metadata": {
        "id": "AbGwH7GiJXdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QvxbTMt3_rV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from aif360.datasets import StandardDataset\n",
        "\n",
        "# Convert test dataset to AIF360 StandardDataset format\n",
        "test_dataset = StandardDataset(\n",
        "    df=pd.concat([X_test_df, y_test], axis=1),  # Ensure protected attribute & label are included\n",
        "    label_name=\"Income_Binary\",  # Target variable\n",
        "    favorable_classes=[1],  # Higher income is favorable\n",
        "    protected_attribute_names=[\"SEX\"],  # Protected attribute (Male/Female)\n",
        "    privileged_classes=[[1]]  # 1 = Male (Privileged)\n",
        ")\n",
        "\n",
        "# Copy the dataset for predictions\n",
        "test_pred_dataset = test_dataset.copy()\n",
        "test_pred_dataset.labels = y_pred.reshape(-1, 1)\n"
      ],
      "metadata": {
        "id": "Xwna3rm_zzDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# Convert AIF360 dataset to DataFrame for visualization\n",
        "df_aif360, _ = test_pred_dataset.convert_to_dataframe()\n",
        "\n",
        "# Plot proportion of high-income predictions per gender\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.barplot(x=\"SEX\", y=\"Income_Binary\", data=df_aif360, estimator=lambda x: sum(x)/len(x))\n",
        "plt.title(\"Proportion of High-Income Predictions by Gender\")\n",
        "plt.xlabel(\"Gender (0 = Female, 1 = Male)\")\n",
        "plt.ylabel(\"Proportion of Predicted High Income\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "iKIoTTuCB6Qg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check unique values in the race column\n",
        "print(\"Unique values in RAC1P column:\", X_test_df[\"RAC1P\"].unique())\n",
        "\n",
        "# Check race distribution in the dataset\n",
        "print(\"\\nProtected Attribute (Race) Distribution in X_test:\")\n",
        "print(X_test_df[\"RAC1P\"].value_counts())\n"
      ],
      "metadata": {
        "id": "sa5YVrSt9HYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Columns in X_test_df:\", X_test_df.columns)\n",
        "\n",
        "# Verify that Income_Binary and RAC1P exist\n",
        "if \"Income_Binary\" not in X_test_df.columns:\n",
        "    print(\"Error: 'Income_Binary' column is missing.\")\n",
        "if \"RAC1P\" not in X_test_df.columns:\n",
        "    print(\"Error: 'RAC1P' column is missing.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "ksLmi9Pu9ZjU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve Income_Binary from the original dataset\n",
        "X_test_df[\"Income_Binary\"] = data.loc[X_test.index, \"Income_Binary\"]\n",
        "\n",
        "# Verify retrieval\n",
        "print(\"Sample Income_Binary values:\", X_test_df[\"Income_Binary\"].head())\n",
        "print(\"Income_Binary distribution:\", X_test_df[\"Income_Binary\"].value_counts())\n"
      ],
      "metadata": {
        "id": "KZVN-yNG-ko6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P5YVkB-A2V_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create DataFrame with predictions\n",
        "df_pred_race = pd.DataFrame({\n",
        "    \"Actual\": X_test_df[\"Income_Binary\"],\n",
        "    \"Predicted\": y_pred_race,\n",
        "    \"Race\": X_test_df[\"RAC1P\"]\n",
        "})\n",
        "\n",
        "# Plot income prediction rate by race\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.barplot(x=\"Race\", y=\"Predicted\", data=df_pred_race, estimator=lambda x: sum(x)/len(x))\n",
        "plt.title(\"Income Prediction Rate by Race\")\n",
        "plt.xlabel(\"Race Category\")\n",
        "plt.ylabel(\"Proportion of High-Income Predictions\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "HSvebNLaAIX9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Define the fairness metrics dictionary for race and income\n",
        "race_metrics = {\n",
        "    \"Disparate Impact\": metric_race.disparate_impact(),\n",
        "    \"Statistical Parity Difference\": metric_race.statistical_parity_difference(),\n",
        "    \"Equal Opportunity Difference\": metric_race.equal_opportunity_difference(),\n",
        "    \"Theil Index\": metric_race.theil_index(),\n",
        "    \"Average Odds Difference\": metric_race.average_odds_difference()\n",
        "}\n",
        "\n",
        "# Define the fairness metrics dictionary for sex and income\n",
        "gender_metrics = {\n",
        "    \"Disparate Impact\": metric.disparate_impact(),\n",
        "    \"Statistical Parity Difference\": metric.statistical_parity_difference(),\n",
        "    \"Equal Opportunity Difference\": metric.equal_opportunity_difference(),\n",
        "    \"Theil Index\": metric.theil_index(),\n",
        "    \"Average Odds Difference\": metric.average_odds_difference()\n",
        "}\n",
        "\n",
        "# Save race metrics to a JSON file\n",
        "with open(r\"C:\\Users\\rohan\\Downloads\\IBM COLLAB\\fairness_metrics_race.json\", \"w\") as f:\n",
        "    json.dump(race_metrics, f, indent=4)\n",
        "\n",
        "# Save gender metrics to a JSON file\n",
        "with open(r\"C:\\Users\\rohan\\Downloads\\IBM COLLAB\\fairness_metrics_gender.json\", \"w\") as f:\n",
        "    json.dump(gender_metrics, f, indent=4)\n",
        "\n",
        "print(\"✅ Fairness metrics saved successfully!\")\n"
      ],
      "metadata": {
        "id": "r5zIkLD0BHsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load race fairness metrics\n",
        "with open(r\"C:\\Users\\rohan\\Downloads\\IBM COLLAB\\fairness_metrics_race.json\", \"r\") as f:\n",
        "    race_metrics_loaded = json.load(f)\n",
        "\n",
        "# Load gender fairness metrics\n",
        "with open(r\"C:\\Users\\rohan\\Downloads\\IBM COLLAB\\fairness_metrics_gender.json\", \"r\") as f:\n",
        "    gender_metrics_loaded = json.load(f)\n",
        "\n",
        "print(\"✅ Loaded Race Fairness Metrics:\", race_metrics_loaded)\n",
        "print(\"✅ Loaded Gender Fairness Metrics:\", gender_metrics_loaded)\n"
      ],
      "metadata": {
        "id": "L_KwzAeOBJ1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "from aif360.datasets import BinaryLabelDataset\n",
        "from aif360.algorithms.preprocessing import Reweighing\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "from aif360.metrics import ClassificationMetric\n",
        "\n",
        "############################################\n",
        "# Step 1: Load Data\n",
        "############################################\n",
        "\n",
        "# Load the training and test splits from a pickle file\n",
        "with open('data_variables.pkl', 'rb') as f:\n",
        "    data_variables = pickle.load(f)\n",
        "\n",
        "X_train = data_variables[\"X_train\"]\n",
        "X_test = data_variables[\"X_test\"]\n",
        "y_train = data_variables[\"y_train\"]\n",
        "y_test = data_variables[\"y_test\"]\n",
        "\n",
        "# Convert training and test features to DataFrames\n",
        "# (If X_train/X_test are already DataFrames with proper column names, skip this conversion)\n",
        "train_df = pd.DataFrame(X_train)\n",
        "test_df = pd.DataFrame(X_test)\n",
        "\n",
        "############################################\n",
        "# Step 2: Ensure Protected Attribute is Included\n",
        "############################################\n",
        "\n",
        "# Check if the SEX column exists; if not, add it.\n",
        "if \"SEX\" not in train_df.columns:\n",
        "    print(\"SEX column not found in training data. Adding dummy SEX values (adjust as needed).\")\n",
        "    # If you have separate protected attributes, replace this dummy assignment\n",
        "    train_df[\"SEX\"] = 1  # For example, assume all are male; replace with actual values.\n",
        "if \"SEX\" not in test_df.columns:\n",
        "    print(\"SEX column not found in test data. Adding dummy SEX values (adjust as needed).\")\n",
        "    test_df[\"SEX\"] = 1\n",
        "\n",
        "# Add target column\n",
        "train_df[\"Income_Binary\"] = y_train\n",
        "test_df[\"Income_Binary\"] = y_test\n",
        "\n",
        "############################################\n",
        "# Step 3: Debug Data Shapes\n",
        "############################################\n",
        "\n",
        "print(\"Training Data Columns:\", train_df.columns.tolist())\n",
        "print(\"Training Data Shape:\", train_df.shape)  # Expecting (num_samples, 12) if there are 11 original features + SEX\n",
        "print(\"Test Data Columns:\", test_df.columns.tolist())\n",
        "print(\"Test Data Shape:\", test_df.shape)\n",
        "\n",
        "############################################\n",
        "# Step 4: Preprocess Protected Attribute\n",
        "############################################\n",
        "\n",
        "# Convert SEX to binary encoding (assuming 1 = Male [privileged], 0 = Female [unprivileged])\n",
        "train_df[\"SEX\"] = train_df[\"SEX\"].apply(lambda x: 1 if x == 1 else 0)\n",
        "test_df[\"SEX\"] = test_df[\"SEX\"].apply(lambda x: 1 if x == 1 else 0)\n",
        "\n",
        "############################################\n",
        "# Step 5: Convert to AIF360 BinaryLabelDataset\n",
        "############################################\n",
        "\n",
        "privileged_groups = [{'SEX': 1}]\n",
        "unprivileged_groups = [{'SEX': 0}]\n",
        "\n",
        "train_dataset = BinaryLabelDataset(\n",
        "    df=train_df,\n",
        "    label_names=[\"Income_Binary\"],\n",
        "    protected_attribute_names=[\"SEX\"],\n",
        "    favorable_label=1,\n",
        "    unfavorable_label=0\n",
        ")\n",
        "\n",
        "test_dataset = BinaryLabelDataset(\n",
        "    df=test_df,\n",
        "    label_names=[\"Income_Binary\"],\n",
        "    protected_attribute_names=[\"SEX\"],\n",
        "    favorable_label=1,\n",
        "    unfavorable_label=0\n",
        ")\n",
        "\n",
        "print(\"AIF360 Training Dataset Features Shape:\", train_dataset.features.shape)\n",
        "print(\"AIF360 Test Dataset Features Shape:\", test_dataset.features.shape)\n",
        "\n",
        "############################################\n",
        "# Step 6: Fix Feature Mismatch if Needed\n",
        "############################################\n",
        "\n",
        "# If the AIF360 dataset features do not include SEX, combine features and protected attributes.\n",
        "# For example, if features shape is (num_samples, 11) but you need 12:\n",
        "if train_dataset.features.shape[1] == 11:\n",
        "    print(\"Features missing protected attribute. Combining features and protected attributes.\")\n",
        "    train_features = np.hstack([train_dataset.features, train_dataset.protected_attributes])\n",
        "    test_features = np.hstack([test_dataset.features, test_dataset.protected_attributes])\n",
        "else:\n",
        "    train_features = train_dataset.features\n",
        "    test_features = test_dataset.features\n",
        "\n",
        "############################################\n",
        "# Step 7: Apply Reweighing to Address Bias\n",
        "############################################\n",
        "\n",
        "RW = Reweighing(privileged_groups=privileged_groups, unprivileged_groups=unprivileged_groups)\n",
        "train_dataset_reweighed = RW.fit_transform(train_dataset)\n",
        "sample_weights = train_dataset_reweighed.instance_weights\n",
        "\n",
        "############################################\n",
        "# Step 8: Train RandomForestClassifier\n",
        "############################################\n",
        "\n",
        "clf_reweighed = RandomForestClassifier(random_state=42, n_estimators=100, max_depth=10)\n",
        "clf_reweighed.fit(train_features, train_dataset.labels.ravel(), sample_weight=sample_weights)\n",
        "\n",
        "print(\"Model expects {} features.\".format(clf_reweighed.n_features_in_))\n",
        "\n",
        "############################################\n",
        "# Step 9: Make Predictions and Evaluate\n",
        "############################################\n",
        "\n",
        "y_pred = clf_reweighed.predict(test_features)\n",
        "accuracy = accuracy_score(test_dataset.labels, y_pred)\n",
        "roc_auc = roc_auc_score(test_dataset.labels, y_pred)\n",
        "\n",
        "print(\"\\n✅ Classifier Accuracy with Reweighing: {:.4f}\".format(accuracy))\n",
        "print(\"ROC-AUC Score: {:.4f}\".format(roc_auc))\n",
        "\n",
        "############################################\n",
        "# Step 10: Evaluate Fairness Metrics\n",
        "############################################\n",
        "\n",
        "# Create a copy of the test dataset with updated predictions\n",
        "test_pred_dataset = test_dataset.copy()\n",
        "test_pred_dataset.labels = y_pred.reshape(-1, 1)\n",
        "\n",
        "metric = ClassificationMetric(\n",
        "    test_dataset,\n",
        "    test_pred_dataset,\n",
        "    unprivileged_groups=unprivileged_groups,\n",
        "    privileged_groups=privileged_groups\n",
        ")\n",
        "\n",
        "print(\"\\n📊 Fairness Metrics with Reweighing:\")\n",
        "print(\"Disparate Impact: {:.4f}\".format(metric.disparate_impact()))\n",
        "print(\"Statistical Parity Difference: {:.4f}\".format(metric.statistical_parity_difference()))\n",
        "print(\"Equal Opportunity Difference: {:.4f}\".format(metric.equal_opportunity_difference()))\n",
        "print(\"Theil Index: {:.4f}\".format(metric.theil_index()))\n",
        "print(\"Average Odds Difference: {:.4f}\".format(metric.average_odds_difference()))\n"
      ],
      "metadata": {
        "id": "pGvT1nc_qUss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training Data Shape:\", train_dataset.features.shape)"
      ],
      "metadata": {
        "id": "Lj9E8zuiGoa7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install joblib"
      ],
      "metadata": {
        "id": "PjaTYdjx9N86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import joblib  # Alternatively, use joblib directly\n",
        "\n",
        "# Save the model using pickle\n",
        "with open('gender_model.pkl', 'wb') as f:\n",
        "    pickle.dump(clf_reweighed, f)\n",
        "\n",
        "# Alternatively, save the model using joblib (more efficient for large models)\n",
        "joblib.dump(clf_reweighed, 'gender_model.joblib')"
      ],
      "metadata": {
        "id": "aLP2O7rf3sB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import joblib  # Alternatively, use joblib directly\n",
        "\n",
        "# Load the model using pickle\n",
        "with open('gender_model.pkl', 'rb') as f:\n",
        "    clf_reweighed_loaded = pickle.load(f)\n",
        "\n",
        "# Alternatively, load the model using joblib\n",
        "clf_reweighed_loaded = joblib.load('gender_model.joblib')"
      ],
      "metadata": {
        "id": "6sGoc2ec43QT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# ✅ Save fairness-aware model results\n",
        "reweighing_results = {\n",
        "    \"Accuracy\": accuracy,\n",
        "    \"ROC-AUC\": roc_auc,\n",
        "    \"Disparate Impact\": metric.disparate_impact(),\n",
        "    \"Statistical Parity Difference\": metric.statistical_parity_difference(),\n",
        "    \"Equal Opportunity Difference\": metric.equal_opportunity_difference(),\n",
        "    \"Theil Index\": metric.theil_index(),\n",
        "    \"Average Odds Difference\": metric.average_odds_difference(),\n",
        "    \"sample_weights\": sample_weights  # Save the instance weights\n",
        "}\n",
        "\n"
      ],
      "metadata": {
        "id": "hYViQ9i_cM6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ✅ Save trained model and results\n",
        "with open(\"reweighing_fairness_results.pkl\", \"wb\") as f:\n",
        "    pickle.dump(reweighing_results, f)\n",
        "\n",
        "with open(\"clf_reweighed.pkl\", \"wb\") as f:\n",
        "    pickle.dump(clf_reweighed, f)\n",
        "\n",
        "print(\"\\n✅ Reweighing Model and Fairness Metrics Saved Successfully!\")"
      ],
      "metadata": {
        "id": "BIV8LhmZoyyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Type of metric:\", type(metric))\n"
      ],
      "metadata": {
        "id": "baB8yYJtcbuu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from aif360.datasets import BinaryLabelDataset\n",
        "from aif360.algorithms.preprocessing import Reweighing\n",
        "from aif360.metrics import ClassificationMetric\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "\n",
        "#########################\n",
        "# PART 1: TRAINING MODEL\n",
        "#########################\n",
        "\n",
        "# Assume 'data' is your full dataset with the target 'Income_Binary' and at least the protected attribute 'SEX'.\n",
        "# Split the dataset into training and test sets.\n",
        "feature_columns = [col for col in data.columns if col != \"Income_Binary\"]\n",
        "X = data[feature_columns]\n",
        "y = data[\"Income_Binary\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Create a training DataFrame that includes the target.\n",
        "train_df = X_train.copy()\n",
        "train_df[\"Income_Binary\"] = y_train\n",
        "\n",
        "# Convert training DataFrame to an AIF360 BinaryLabelDataset (using SEX as the protected attribute for reweighing).\n",
        "train_dataset = BinaryLabelDataset(\n",
        "    df=train_df,\n",
        "    label_names=[\"Income_Binary\"],\n",
        "    protected_attribute_names=[\"SEX\"],\n",
        "    favorable_label=1,\n",
        "    unfavorable_label=0\n",
        ")\n",
        "\n",
        "# Define privileged and unprivileged groups for SEX.\n",
        "# (Assume SEX: 1 = male (privileged), 2 = female (unprivileged))\n",
        "privileged_groups = [{'SEX': 1}]\n",
        "unprivileged_groups = [{'SEX': 2}]\n",
        "\n",
        "# Apply Reweighing for fairness mitigation on SEX.\n",
        "RW = Reweighing(privileged_groups=privileged_groups, unprivileged_groups=unprivileged_groups)\n",
        "train_dataset_reweighed = RW.fit_transform(train_dataset)\n",
        "sample_weights = train_dataset_reweighed.instance_weights\n",
        "\n",
        "# Train a binary classifier using the reweighed data.\n",
        "clf_reweighed = RandomForestClassifier(random_state=42, n_estimators=100, max_depth=10)\n",
        "clf_reweighed.fit(X_train, y_train, sample_weight=sample_weights)\n",
        "\n",
        "# Evaluate model performance on the test set.\n",
        "X_test_df = X_test.copy()\n",
        "X_test_df[\"Income_Binary\"] = y_test  # Add target for evaluation.\n",
        "y_pred = clf_reweighed.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, y_pred)\n",
        "print(\"Model Performance:\")\n",
        "print(f\"  Accuracy: {accuracy:.4f}\")\n",
        "print(f\"  ROC-AUC: {roc_auc:.4f}\")\n",
        "\n",
        "#########################\n",
        "# PART 2: FAIRNESS EVALUATION\n",
        "#########################\n",
        "\n",
        "# Create separate AIF360 BinaryLabelDataset objects for each protected attribute.\n",
        "# We will evaluate fairness for:\n",
        "#   - Gender: using the \"SEX\" column.\n",
        "#   - Race: using the \"RAC1P\" column.\n",
        "#   - Age: using the \"AGEP\" column.\n",
        "#   - Marital Status: using the \"MAR\" column.\n",
        "\n",
        "test_dataset_sex = BinaryLabelDataset(\n",
        "    df=X_test_df,\n",
        "    label_names=[\"Income_Binary\"],\n",
        "    protected_attribute_names=[\"SEX\"],\n",
        "    favorable_label=1,\n",
        "    unfavorable_label=0\n",
        ")\n",
        "\n",
        "test_dataset_race = BinaryLabelDataset(\n",
        "    df=X_test_df,\n",
        "    label_names=[\"Income_Binary\"],\n",
        "    protected_attribute_names=[\"RAC1P\"],\n",
        "    favorable_label=1,\n",
        "    unfavorable_label=0\n",
        ")\n",
        "\n",
        "test_dataset_age = BinaryLabelDataset(\n",
        "    df=X_test_df,\n",
        "    label_names=[\"Income_Binary\"],\n",
        "    protected_attribute_names=[\"AGEP\"],\n",
        "    favorable_label=1,\n",
        "    unfavorable_label=0\n",
        ")\n",
        "\n",
        "test_dataset_mar = BinaryLabelDataset(\n",
        "    df=X_test_df,\n",
        "    label_names=[\"Income_Binary\"],\n",
        "    protected_attribute_names=[\"MAR\"],\n",
        "    favorable_label=1,\n",
        "    unfavorable_label=0\n",
        ")\n",
        "\n",
        "# Generate predictions using the trained model.\n",
        "# IMPORTANT: Ensure that the features passed to the model match what was used during training.\n",
        "# Drop only the target column (\"Income_Binary\") to keep the original feature set.\n",
        "X_test_features = X_test_df.drop(columns=[\"Income_Binary\"])\n",
        "y_pred = clf_reweighed.predict(X_test_features)\n",
        "\n",
        "# Create predicted datasets by copying the test datasets and updating labels.\n",
        "test_pred_dataset_sex = test_dataset_sex.copy()\n",
        "test_pred_dataset_sex.labels = y_pred.reshape(-1, 1)\n",
        "\n",
        "test_pred_dataset_race = test_dataset_race.copy()\n",
        "test_pred_dataset_race.labels = y_pred.reshape(-1, 1)\n",
        "\n",
        "test_pred_dataset_age = test_dataset_age.copy()\n",
        "test_pred_dataset_age.labels = y_pred.reshape(-1, 1)\n",
        "\n",
        "test_pred_dataset_mar = test_dataset_mar.copy()\n",
        "test_pred_dataset_mar.labels = y_pred.reshape(-1, 1)\n",
        "\n",
        "# Define privileged and unprivileged groups for each attribute.\n",
        "# Gender: Privileged = male (SEX = 1), Unprivileged = female (SEX = 2)\n",
        "privileged_groups_sex = [{\"SEX\": 1}]\n",
        "unprivileged_groups_sex = [{\"SEX\": 2}]\n",
        "\n",
        "# Race: Privileged = White (RAC1P = 1), Unprivileged = non-White (RAC1P = 2,3,...,9)\n",
        "privileged_groups_race = [{\"RAC1P\": 1}]\n",
        "unprivileged_groups_race = [{\"RAC1P\": i} for i in range(2, 10)]\n",
        "\n",
        "# Age: Define groups based on median age.\n",
        "median_age = X_test_df[\"AGEP\"].median()\n",
        "privileged_groups_age = [{\"AGEP\": i} for i in X_test_df[\"AGEP\"].unique() if i >= median_age]\n",
        "unprivileged_groups_age = [{\"AGEP\": i} for i in X_test_df[\"AGEP\"].unique() if i < median_age]\n",
        "\n",
        "# Marital Status: Privileged = Married (MAR = 1.0), Unprivileged = all other values.\n",
        "privileged_groups_mar = [{\"MAR\": 1.0}]\n",
        "unprivileged_groups_mar = [{\"MAR\": i} for i in X_test_df[\"MAR\"].unique() if i != 1.0]\n",
        "\n",
        "# Compute fairness metrics for each protected attribute.\n",
        "metric_sex = ClassificationMetric(test_dataset_sex, test_pred_dataset_sex,\n",
        "                                  privileged_groups=privileged_groups_sex,\n",
        "                                  unprivileged_groups=unprivileged_groups_sex)\n",
        "\n",
        "metric_race = ClassificationMetric(test_dataset_race, test_pred_dataset_race,\n",
        "                                   privileged_groups=privileged_groups_race,\n",
        "                                   unprivileged_groups=unprivileged_groups_race)\n",
        "\n",
        "metric_age = ClassificationMetric(test_dataset_age, test_pred_dataset_age,\n",
        "                                  privileged_groups=privileged_groups_age,\n",
        "                                  unprivileged_groups=unprivileged_groups_age)\n",
        "\n",
        "metric_mar = ClassificationMetric(test_dataset_mar, test_pred_dataset_mar,\n",
        "                                  privileged_groups=privileged_groups_mar,\n",
        "                                  unprivileged_groups=unprivileged_groups_mar)\n",
        "\n",
        "# Print fairness metrics for each attribute.\n",
        "print(\"\\nFairness Metrics for Gender (SEX):\")\n",
        "print(f\"  Disparate Impact: {metric_sex.disparate_impact():.4f}\")\n",
        "print(f\"  Statistical Parity Difference: {metric_sex.statistical_parity_difference():.4f}\")\n",
        "print(f\"  Equal Opportunity Difference: {metric_sex.equal_opportunity_difference():.4f}\")\n",
        "print(f\"  Theil Index: {metric_sex.theil_index():.4f}\")\n",
        "print(f\"  Average Odds Difference: {metric_sex.average_odds_difference():.4f}\")\n",
        "\n",
        "print(\"\\nFairness Metrics for Race (RAC1P):\")\n",
        "print(f\"  Disparate Impact: {metric_race.disparate_impact():.4f}\")\n",
        "print(f\"  Statistical Parity Difference: {metric_race.statistical_parity_difference():.4f}\")\n",
        "print(f\"  Equal Opportunity Difference: {metric_race.equal_opportunity_difference():.4f}\")\n",
        "print(f\"  Theil Index: {metric_race.theil_index():.4f}\")\n",
        "print(f\"  Average Odds Difference: {metric_race.average_odds_difference():.4f}\")\n",
        "\n",
        "print(\"\\nFairness Metrics for Age (AGEP):\")\n",
        "print(f\"  Disparate Impact: {metric_age.disparate_impact():.4f}\")\n",
        "print(f\"  Statistical Parity Difference: {metric_age.statistical_parity_difference():.4f}\")\n",
        "print(f\"  Equal Opportunity Difference: {metric_age.equal_opportunity_difference():.4f}\")\n",
        "print(f\"  Theil Index: {metric_age.theil_index():.4f}\")\n",
        "print(f\"  Average Odds Difference: {metric_age.average_odds_difference():.4f}\")\n",
        "\n",
        "print(\"\\nFairness Metrics for Marital Status (MAR):\")\n",
        "print(f\"  Disparate Impact: {metric_mar.disparate_impact():.4f}\")\n",
        "print(f\"  Statistical Parity Difference: {metric_mar.statistical_parity_difference():.4f}\")\n",
        "print(f\"  Equal Opportunity Difference: {metric_mar.equal_opportunity_difference():.4f}\")\n",
        "print(f\"  Theil Index: {metric_mar.theil_index():.4f}\")\n",
        "print(f\"  Average Odds Difference: {metric_mar.average_odds_difference():.4f}\")\n"
      ],
      "metadata": {
        "id": "6N1Y9S4VxiGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overall Interpretation and Trade-Offs\n",
        "Fairness Improvements:\n",
        "The reweighing technique has improved fairness. The Disparate Impact is close to 1, and the Statistical Parity Difference is near 0, both of which indicate that the favorable outcome (high income) is distributed fairly across groups defined by the protected attribute (e.g., gender if SEX is used).\n",
        "\n",
        "Performance vs. Fairness:\n",
        "With an accuracy of about 79% and ROC-AUC around 0.76, the model maintains reasonable performance while achieving improved fairness. The trade-off appears acceptable in this case—fairness metrics indicate low disparity, and performance metrics remain in a moderate range"
      ],
      "metadata": {
        "id": "O-PjeNBHvqE2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gc\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from aif360.datasets import BinaryLabelDataset\n",
        "from aif360.algorithms.inprocessing import AdversarialDebiasing\n",
        "from aif360.metrics import ClassificationMetric\n",
        "\n",
        "# ------------------------------------------\n",
        "# Step 1: Set up TensorFlow session\n",
        "# ------------------------------------------\n",
        "tf.compat.v1.reset_default_graph()\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "sess = tf.compat.v1.Session()\n",
        "\n",
        "# ------------------------------------------\n",
        "# Step 2: Load the Data (Ensure X_test_df is defined)\n",
        "# ------------------------------------------\n",
        "X_test_df = X_test_df.dropna(subset=['MAR', 'Income_Binary'])\n",
        "\n",
        "# Binarize MAR (1.0 for married, 0.0 otherwise)\n",
        "X_test_df['MAR'] = X_test_df['MAR'].apply(lambda x: 1.0 if x == 1.0 else 0.0)\n",
        "\n",
        "# Convert DataFrame to AIF360 BinaryLabelDataset\n",
        "dataset_marriage = BinaryLabelDataset(\n",
        "    df=X_test_df,\n",
        "    label_names=['Income_Binary'],\n",
        "    protected_attribute_names=['MAR'],\n",
        "    favorable_label=1,\n",
        "    unfavorable_label=0\n",
        ")\n",
        "\n",
        "# Normalize Features\n",
        "scaler = StandardScaler()\n",
        "dataset_marriage.features = scaler.fit_transform(dataset_marriage.features)\n",
        "\n",
        "# Define privileged and unprivileged groups\n",
        "privileged_groups = [{'MAR': 1.0}]\n",
        "unprivileged_groups = [{'MAR': 0.0}]\n",
        "\n",
        "# Split into 70% training, 30% testing (same split strategy)\n",
        "train_marriage, test_marriage = dataset_marriage.split([0.7], shuffle=True, seed=42)\n",
        "\n",
        "# ------------------------------------------\n",
        "# Step 3: Train the Adversarial Debiasing Model\n",
        "# ------------------------------------------\n",
        "debiased_model = AdversarialDebiasing(\n",
        "    unprivileged_groups=unprivileged_groups,\n",
        "    privileged_groups=privileged_groups,\n",
        "    scope_name='debiased_marriage',\n",
        "    debias=True,\n",
        "    num_epochs=20,\n",
        "    batch_size=256,\n",
        "    sess=sess\n",
        ")\n",
        "\n",
        "debiased_model.fit(train_marriage)\n",
        "\n",
        "# ------------------------------------------\n",
        "# Step 4: Save the Model as \"marriage_model.ckpt\"\n",
        "# ------------------------------------------\n",
        "saver = tf.compat.v1.train.Saver()\n",
        "save_path = saver.save(sess, \"marriage_model.ckpt\")\n",
        "print(\"✅ Model saved as 'marriage_model.ckpt'\")\n",
        "\n",
        "# Close the TensorFlow session after saving\n",
        "sess.close()\n"
      ],
      "metadata": {
        "id": "cKKvhbprRAYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JHIXOeKJSX6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Load the final metrics from the file.\n",
        "with open('final_metrics_mar.pkl', 'rb') as f:\n",
        "    loaded_metrics = pickle.load(f)\n",
        "\n",
        "print(\"Loaded Final Metrics for MAR:\")\n",
        "print(loaded_metrics)\n"
      ],
      "metadata": {
        "id": "-O5z25dCXPVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Example: Check counts for SEX and Income_Binary in your DataFrame\n",
        "print(data['SEX'].value_counts())\n",
        "print(data.groupby('SEX')['Income_Binary'].value_counts())\n"
      ],
      "metadata": {
        "id": "XF1k9-PEmYAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()"
      ],
      "metadata": {
        "id": "JA2dm1_m2Ayb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib  # Use joblib for saving/loading models\n",
        "\n",
        "# Save the trained model\n",
        "joblib.dump(clf, 'logistic_regression_model.joblib')\n",
        "\n",
        "# Save the scaler (if needed for later transformations)\n",
        "joblib.dump(scaler, 'scaler.joblib')"
      ],
      "metadata": {
        "id": "wnzvE_UeAEDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Load the trained model\n",
        "clf_loaded = joblib.load('logistic_regression_model.joblib')\n",
        "\n",
        "# Load the scaler (if needed)\n",
        "scaler_loaded = joblib.load('scaler.joblib')"
      ],
      "metadata": {
        "id": "uV_rMZCrAGJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# ✅ Save fairness metrics and accuracy results\n",
        "results = {\n",
        "    \"Accuracy\": accuracy,\n",
        "    \"Disparate Impact\": metrics.disparate_impact(),\n",
        "    \"Equal Opportunity Difference\": metrics.equal_opportunity_difference(),\n",
        "    \"Average Odds Difference\": metrics.average_odds_difference(),\n",
        "    \"Statistical Parity Difference\": metrics.statistical_parity_difference(),\n",
        "    \"Unique Predictions\": np.unique(pred_labels, return_counts=True),\n",
        "    \"Privileged Group Positive Predictions\": (test_dataset.labels[(test_dataset.protected_attributes[:, 0] == 1)] == 1).sum(),\n",
        "    \"Unprivileged Group Positive Predictions\": (test_dataset.labels[(test_dataset.protected_attributes[:, 0] == 0)] == 1).sum(),\n",
        "    \"SEX Unique Values\": np.unique(test_dataset.protected_attributes)\n",
        "}\n",
        "\n",
        "# Save to a pickle file\n",
        "with open(\"reweighing_fairness_results.pkl\", \"wb\") as f:\n",
        "    pickle.dump(results, f)\n",
        "\n",
        "print(\"✅ Fairness metrics saved to 'reweighing_fairness_results.pkl'\")\n"
      ],
      "metadata": {
        "id": "vzFZUybRMis4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load fairness results\n",
        "with open(\"reweighing_fairness_results.pkl\", \"rb\") as f:\n",
        "    loaded_results = pickle.load(f)\n",
        "\n",
        "# Print results\n",
        "print(\"📊 Loaded Fairness Metrics and Model Performance:\")\n",
        "for key, value in loaded_results.items():\n",
        "    print(f\"{key}: {value}\")\n"
      ],
      "metadata": {
        "id": "heMVZkoGMksS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a binary indicator column where 1 indicates 'Married'\n",
        "X_test_df['is_married'] = X_test_df['MAR'].apply(lambda x: 1 if x == \"Married\" else 0)\n"
      ],
      "metadata": {
        "id": "1JgAczaQq_cN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a binary indicator: 1 if White alone, 0 otherwise.\n",
        "X_test_df['is_white'] = X_test_df['RAC1P'].apply(lambda x: 1 if x == 1 else 0)\n"
      ],
      "metadata": {
        "id": "0ApoEq1ahzPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "from aif360.datasets import BinaryLabelDataset\n",
        "from aif360.algorithms.preprocessing import Reweighing\n",
        "from aif360.metrics import ClassificationMetric\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "############################################\n",
        "# Step 0: Data Preparation and Feature Engineering\n",
        "############################################\n",
        "\n",
        "# (Assuming X_test_df is defined and contains at least 'RAC1P' and 'Income_Binary')\n",
        "# Create a binary race indicator: treat \"White alone\" (code 1) as privileged.\n",
        "X_test_df['is_white'] = X_test_df['RAC1P'].apply(lambda x: 1 if x == 1 else 0)\n",
        "\n",
        "############################################\n",
        "# Step 1: Create an AIF360 Dataset\n",
        "############################################\n",
        "\n",
        "dataset_race = BinaryLabelDataset(\n",
        "    df=X_test_df,\n",
        "    label_names=['Income_Binary'],\n",
        "    protected_attribute_names=['is_white'],\n",
        "    favorable_label=1,\n",
        "    unfavorable_label=0\n",
        ")\n",
        "\n",
        "# Define protected groups\n",
        "privileged_groups = [{'is_white': 1}]\n",
        "unprivileged_groups = [{'is_white': 0}]\n",
        "\n",
        "############################################\n",
        "# Step 2: Apply the Reweighing Algorithm\n",
        "############################################\n",
        "\n",
        "rw = Reweighing(privileged_groups=privileged_groups, unprivileged_groups=unprivileged_groups)\n",
        "dataset_race_rw = rw.fit_transform(dataset_race)\n",
        "\n",
        "############################################\n",
        "# Step 3: Split Data for Training and Testing\n",
        "############################################\n",
        "\n",
        "train_race, test_race = dataset_race_rw.split([0.7], shuffle=True, seed=42)\n",
        "\n",
        "############################################\n",
        "# Step 4: Combine Features with Protected Attributes\n",
        "############################################\n",
        "# Note: BinaryLabelDataset removes the protected attribute from the features.\n",
        "# To include it, we explicitly combine them.\n",
        "train_features = np.hstack([train_race.features, train_race.protected_attributes])\n",
        "test_features = np.hstack([test_race.features, test_race.protected_attributes])\n",
        "\n",
        "print(\"Training features shape:\", train_features.shape)\n",
        "print(\"Test features shape:\", test_features.shape)\n",
        "\n",
        "############################################\n",
        "# Step 5: Train and Save the RandomForestClassifier Model\n",
        "############################################\n",
        "\n",
        "# Define model save directory\n",
        "MODEL_DIR = \"C:/Users/rohan/Downloads/IBM COLLAB\"\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "\n",
        "clf_race = RandomForestClassifier(random_state=42, n_estimators=100, max_depth=10)\n",
        "clf_race.fit(train_features, train_race.labels.ravel(), sample_weight=train_race.instance_weights)\n",
        "\n",
        "print(\"Model expects {} features.\".format(clf_race.n_features_in_))\n",
        "\n",
        "racist_model_path = os.path.join(MODEL_DIR, \"racist_model.pkl\")\n",
        "with open(racist_model_path, 'wb') as f:\n",
        "    pickle.dump(clf_race, f)\n",
        "\n",
        "print(f\"✅ Model saved successfully at: {racist_model_path}\")\n",
        "\n",
        "############################################\n",
        "# Step 6: Load the Model and Generate Predictions\n",
        "############################################\n",
        "\n",
        "try:\n",
        "    with open(racist_model_path, 'rb') as f:\n",
        "        loaded_clf_race = pickle.load(f)\n",
        "    print(f\"✅ Model loaded successfully from: {racist_model_path}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"❌ Error: Model file not found at {racist_model_path}.\")\n",
        "\n",
        "# Use the combined test_features (with the protected attribute) for prediction\n",
        "pred_labels_race = loaded_clf_race.predict(test_features)\n",
        "accuracy = accuracy_score(test_race.labels, pred_labels_race)\n",
        "print(f\"🎯 Accuracy on Test Data: {accuracy:.4f}\")\n",
        "\n",
        "############################################\n",
        "# Step 7: Compute Fairness Metrics\n",
        "############################################\n",
        "\n",
        "# Create a copy of the test dataset and update labels with model predictions\n",
        "test_race_pred = test_race.copy()\n",
        "test_race_pred.labels = pred_labels_race.reshape(-1, 1)\n",
        "\n",
        "metrics = ClassificationMetric(\n",
        "    test_race,            # Dataset with true labels\n",
        "    test_race_pred,       # Dataset with predicted labels\n",
        "    unprivileged_groups=unprivileged_groups,\n",
        "    privileged_groups=privileged_groups\n",
        ")\n",
        "\n",
        "print(\"\\n📊 FINAL RACE FAIRNESS METRICS\")\n",
        "print(f\"Disparate Impact:             {metrics.disparate_impact():.4f}\")\n",
        "print(f\"Equal Opportunity Difference: {metrics.equal_opportunity_difference():.4f}\")\n",
        "print(f\"Average Odds Difference:      {metrics.average_odds_difference():.4f}\")\n",
        "print(f\"Statistical Parity Difference:{metrics.statistical_parity_difference():.4f}\")\n"
      ],
      "metadata": {
        "id": "KI5E5WpIF1t6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "# (Assume clf_race is your trained LogisticRegression model.)\n",
        "\n",
        "# Save the trained model to a file named \"racist_model.pkl\"\n",
        "with open('racist_model.pkl', 'wb') as f:\n",
        "    pickle.dump(clf_race, f)\n",
        "print(\"Model saved to 'racist_model.pkl'.\")\n"
      ],
      "metadata": {
        "id": "rwTDL2yUE34R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Load the model from \"racist_model.pkl\"\n",
        "with open('racist_model.pkl', 'rb') as f:\n",
        "    loaded_clf_race = pickle.load(f)\n",
        "print(\"Model loaded from 'racist_model.pkl'.\")\n"
      ],
      "metadata": {
        "id": "NtNErAeME8P4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import os\n",
        "\n",
        "# Define the model directory\n",
        "MODEL_DIR = \"C:/Users/rohan/Downloads/IBM COLLAB\"\n",
        "racist_model_path = os.path.join(MODEL_DIR, \"racist_model.pkl\")\n",
        "\n",
        "# Load the model\n",
        "try:\n",
        "    with open(racist_model_path, 'rb') as f:\n",
        "        loaded_clf_race = pickle.load(f)\n",
        "    print(f\"✅ Model loaded successfully from '{racist_model_path}'.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"❌ Error: File not found at '{racist_model_path}'. Make sure the model is saved first.\")\n"
      ],
      "metadata": {
        "id": "HTw_qjt2oKLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from joblib import dump\n",
        "\n",
        "# Save the trained model to a file\n",
        "dump(clf_race, 'logistic_regression_model.joblib')\n",
        "print(\"Model saved to 'logistic_regression_model.joblib'\")"
      ],
      "metadata": {
        "id": "OdHw9QRxJpaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from joblib import load\n",
        "\n",
        "# Load the model from the file\n",
        "clf_race_loaded = load('logistic_regression_model.joblib')\n",
        "print(\"Model loaded from 'logistic_regression_model.joblib'\")"
      ],
      "metadata": {
        "id": "11xOd4XYFWqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the test set using the loaded model\n",
        "pred_labels_race_loaded = clf_race_loaded.predict(test_race.features)\n",
        "\n",
        "# Compute accuracy (or other metrics) using the loaded model\n",
        "accuracy_loaded = accuracy_score(test_race.labels, pred_labels_race_loaded)\n",
        "print(\"Accuracy on Test Data (using loaded model): {:.4f}\".format(accuracy_loaded))"
      ],
      "metadata": {
        "id": "MAnQ9FtXFf2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " import os\n",
        "os.environ[\"PYTORCH_DYNAMO\"] = \"0\"  # Disable TorchDynamo\n",
        "\n",
        "import torch# Disable Dynamo before importing PyTorch\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "from aif360.datasets import BinaryLabelDataset\n",
        "from aif360.metrics import ClassificationMetric\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Rest of your code remains the same...\n",
        "\n",
        "# Disable PyTorch dynamo for performance optimization\n",
        "os.environ[\"PYTORCH_DYNAMO\"] = \"0\"\n",
        "\n",
        "# Define the directory to save the model\n",
        "MODEL_DIR = \"C:/Users/rohan/Downloads/IBM COLLAB\"\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)  # Ensure the directory exists\n",
        "\n",
        "# ====================================================\n",
        "# Step 0: Data Preparation and Feature Engineering\n",
        "# ====================================================\n",
        "# Assuming X_test_df is already defined\n",
        "df = X_test_df.copy()\n",
        "\n",
        "# Create a binary age indicator\n",
        "df['is_privileged_age'] = df['AGEP'].apply(lambda x: 1 if 20 <= x <= 60 else 0)\n",
        "\n",
        "# Feature columns (exclude target and sensitive columns)\n",
        "feature_cols = df.columns.difference(['Income_Binary', 'AGEP', 'is_privileged_age'])\n",
        "X = df[feature_cols].values.astype(np.float32)\n",
        "y = df['Income_Binary'].values.astype(np.float32)\n",
        "sensitive = df['is_privileged_age'].values.astype(np.float32)\n",
        "\n",
        "# ====================================================\n",
        "# Step 1: Split the Data into Training, Validation, and Testing Sets\n",
        "# ====================================================\n",
        "X_train, X_test, y_train, y_test, s_train, s_test = train_test_split(\n",
        "    X, y, sensitive, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "\n",
        "# Convert numpy arrays to PyTorch tensors and create DataLoaders\n",
        "train_data = TensorDataset(torch.tensor(X_train), torch.tensor(y_train).view(-1, 1), torch.tensor(s_train).view(-1, 1))\n",
        "val_data = TensorDataset(torch.tensor(X_val), torch.tensor(y_val).view(-1, 1), torch.tensor(s_val).view(-1, 1))\n",
        "test_data = TensorDataset(torch.tensor(X_test), torch.tensor(y_test).view(-1, 1), torch.tensor(s_test).view(-1, 1))\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=64, shuffle=False)\n",
        "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)\n",
        "\n",
        "# ====================================================\n",
        "# Step 2: Define a Fairness-Aware Logistic Regression Model\n",
        "# ====================================================\n",
        "class FairLogisticRegression(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(FairLogisticRegression, self).__init__()\n",
        "        self.linear = nn.Linear(input_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.sigmoid(self.linear(x))\n",
        "\n",
        "input_dim = X_train.shape[1]\n",
        "model = FairLogisticRegression(input_dim)\n",
        "\n",
        "# ====================================================\n",
        "# Step 3: Define Loss Function with Fairness Regularization\n",
        "# ====================================================\n",
        "criterion = nn.BCELoss()\n",
        "learning_rate = 0.001      # Reduced learning rate\n",
        "fairness_lambda = 0.01     # Reduced fairness regularization weight\n",
        "weight_decay = 1e-4        # Increased L2 regularization\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)\n",
        "\n",
        "# ====================================================\n",
        "# Step 4: Train the Model with Fairness-Aware Regularization\n",
        "# ====================================================\n",
        "num_epochs = 100\n",
        "best_val_loss = float('inf')\n",
        "patience = 5\n",
        "trigger_times = 0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "    for X_batch, y_batch, s_batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(X_batch)\n",
        "        loss_bce = criterion(outputs, y_batch)\n",
        "\n",
        "        # Fairness regularization\n",
        "        priv_mask = (s_batch == 1)\n",
        "        unpriv_mask = (s_batch == 0)\n",
        "        if priv_mask.sum() > 0 and unpriv_mask.sum() > 0:\n",
        "            priv_mean = outputs[priv_mask].mean()\n",
        "            unpriv_mean = outputs[unpriv_mask].mean()\n",
        "            fairness_penalty = (priv_mean - unpriv_mean) ** 2\n",
        "        else:\n",
        "            fairness_penalty = torch.tensor(0.0)\n",
        "\n",
        "        loss = loss_bce + fairness_lambda * fairness_penalty\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    # Validation Loss\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch, s_batch in val_loader:\n",
        "            outputs = model(X_batch)\n",
        "            loss_bce = criterion(outputs, y_batch)\n",
        "            val_loss += loss_bce.item()\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {epoch_loss/len(train_loader):.4f}, Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "    # Early Stopping\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        trigger_times = 0\n",
        "    else:\n",
        "        trigger_times += 1\n",
        "        if trigger_times >= patience:\n",
        "            print(\"Early stopping!\")\n",
        "            break\n",
        "\n",
        "# ====================================================\n",
        "# Step 5: Evaluate the Model on Test Data\n",
        "# ====================================================\n",
        "model.eval()\n",
        "y_pred_list = []\n",
        "with torch.no_grad():\n",
        "    for X_batch, y_batch, _ in test_loader:\n",
        "        y_pred_prob = model(X_batch)\n",
        "        y_pred = (y_pred_prob >= 0.5).float()\n",
        "        y_pred_list.append(y_pred)\n",
        "\n",
        "y_pred = torch.cat(y_pred_list).numpy()\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "test_f1 = f1_score(y_test, y_pred)\n",
        "test_precision = precision_score(y_test, y_pred)\n",
        "test_recall = recall_score(y_test, y_pred)\n",
        "\n",
        "print(f\"\\nTest Metrics:\")\n",
        "print(f\"Accuracy: {test_accuracy:.4f}\")\n",
        "print(f\"F1-Score: {test_f1:.4f}\")\n",
        "print(f\"Precision: {test_precision:.4f}\")\n",
        "print(f\"Recall: {test_recall:.4f}\")\n",
        "\n",
        "# ====================================================\n",
        "# Step 6: Fairness Evaluation Using AIF360\n",
        "# ====================================================\n",
        "test_df = pd.DataFrame(X_test, columns=feature_cols)\n",
        "test_df['Income_Binary'] = y_test\n",
        "test_df['is_privileged_age'] = s_test\n",
        "test_df['pred'] = y_pred\n",
        "\n",
        "dataset_true = BinaryLabelDataset(\n",
        "    df=test_df,\n",
        "    label_names=['Income_Binary'],\n",
        "    protected_attribute_names=['is_privileged_age'],\n",
        "    favorable_label=1,\n",
        "    unfavorable_label=0\n",
        ")\n",
        "\n",
        "dataset_pred = dataset_true.copy()\n",
        "dataset_pred.labels = test_df['pred'].values.reshape(-1, 1)\n",
        "\n",
        "privileged_groups = [{'is_privileged_age': 1}]\n",
        "unprivileged_groups = [{'is_privileged_age': 0}]\n",
        "\n",
        "metrics = ClassificationMetric(\n",
        "    dataset_true,\n",
        "    dataset_pred,\n",
        "    unprivileged_groups=unprivileged_groups,\n",
        "    privileged_groups=privileged_groups\n",
        ")\n",
        "\n",
        "print(\"\\nFairness Metrics:\")\n",
        "print(\"Disparate Impact:             {:.4f}\".format(metrics.disparate_impact()))\n",
        "print(\"Equal Opportunity Difference: {:.4f}\".format(metrics.equal_opportunity_difference()))\n",
        "print(\"Average Odds Difference:      {:.4f}\".format(metrics.average_odds_difference()))\n",
        "print(\"Statistical Parity Difference:{:.4f}\".format(metrics.statistical_parity_difference()))\n",
        "\n",
        "# ====================================================\n",
        "# Step 7: Save the Trained Age Model\n",
        "# ====================================================\n",
        "age_model_save_path = os.path.join(MODEL_DIR, \"age_model.pth\")\n",
        "torch.save(model.state_dict(), age_model_save_path)\n",
        "print(f\"✅ Age model saved successfully at {age_model_save_path}\")"
      ],
      "metadata": {
        "id": "E8e_LTWLMuZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade torch torchvision torchaudio"
      ],
      "metadata": {
        "id": "0ZogG_UtDF4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(\"PyTorch version:\", torch.__version__)"
      ],
      "metadata": {
        "id": "QUEgIQ8sFjQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade torch torchvision torchaudio\n",
        "!pip install --upgrade functorch"
      ],
      "metadata": {
        "id": "H2fwCIfrEmV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"PYTORCH_DYNAMO\"] = \"0\"\n",
        "import torch\n"
      ],
      "metadata": {
        "id": "oSgxMFI_Beek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade torch\n"
      ],
      "metadata": {
        "id": "iIdKNGmC_Z2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Save the trained model\n",
        "torch.save(model.state_dict(), \"age_model.pth\")\n",
        "print(\"✅ Model successfully saved as 'age_model.pth'.\")\n"
      ],
      "metadata": {
        "id": "uhlnjgQpbqrk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SvblFARbP4OR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import torch\n",
        "\n",
        "# ✅ Save fairness metrics and model performance\n",
        "results = {\n",
        "    \"Test Accuracy\": test_accuracy,\n",
        "    \"Test F1-Score\": test_f1,\n",
        "    \"Test Precision\": test_precision,\n",
        "    \"Test Recall\": test_recall,\n",
        "    \"Disparate Impact\": metrics.disparate_impact(),\n",
        "    \"Equal Opportunity Difference\": metrics.equal_opportunity_difference(),\n",
        "    \"Average Odds Difference\": metrics.average_odds_difference(),\n",
        "    \"Statistical Parity Difference\": metrics.statistical_parity_difference()\n",
        "}\n",
        "\n",
        "# ✅ Save model state and results to a file\n",
        "with open(\"fairness_model_results.pkl\", \"wb\") as f:\n",
        "    pickle.dump(results, f)\n",
        "\n",
        "# ✅ Save PyTorch model separately\n",
        "torch.save(model.state_dict(), \"fairness_model.pth\")\n",
        "\n",
        "print(\"✅ Model and fairness metrics saved successfully!\")\n"
      ],
      "metadata": {
        "id": "M79_Zh59Q9MQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Load saved fairness metrics\n",
        "with open(\"fairness_model_results.pkl\", \"rb\") as f:\n",
        "    loaded_results = pickle.load(f)\n",
        "\n",
        "# ✅ Print saved results\n",
        "print(\"\\n📊 Loaded Fairness Metrics and Model Performance:\")\n",
        "for key, value in loaded_results.items():\n",
        "    print(f\"{key}: {value}\")\n",
        "\n",
        "# ✅ Load PyTorch model\n",
        "input_dim = X_test.shape[1]  # Ensure correct input dimension\n",
        "loaded_model = FairLogisticRegression(input_dim)\n",
        "loaded_model.load_state_dict(torch.load(\"fairness_model.pth\"))\n",
        "loaded_model.eval()\n",
        "\n",
        "print(\"\\n✅ Model loaded successfully and ready for inference!\")\n"
      ],
      "metadata": {
        "id": "b419B7QTQ-qz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Class Distribution\n",
        "print(\"Class Distribution in Training Set:\")\n",
        "print(pd.Series(y_train).value_counts(normalize=True))\n",
        "\n",
        "print(\"\\nClass Distribution in Test Set:\")\n",
        "print(pd.Series(y_test).value_counts(normalize=True))\n",
        "\n",
        "# Evaluate on a Simpler Model (Logistic Regression without Fairness Regularization)\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "simple_model = LogisticRegression()\n",
        "simple_model.fit(X_train, y_train)\n",
        "y_pred_simple = simple_model.predict(X_test)\n",
        "\n",
        "print(\"\\nSimple Model Test Metrics:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred_simple):.4f}\")\n",
        "print(f\"F1-Score: {f1_score(y_test, y_pred_simple):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred_simple):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test, y_pred_simple):.4f}\")"
      ],
      "metadata": {
        "id": "YD-aHkdVa8n6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "model_save_path = \"fair_logistic_regression_model.pth\"\n",
        "torch.save(model.state_dict(), model_save_path)\n",
        "print(f\"Model saved to {model_save_path}\")"
      ],
      "metadata": {
        "id": "utLuIY44bWZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model\n",
        "model_load_path = \"fair_logistic_regression_model.pth\"\n",
        "loaded_model = FairLogisticRegression(input_dim)  # Create a new instance of the model\n",
        "loaded_model.load_state_dict(torch.load(model_load_path))\n",
        "loaded_model.eval()  # Set the model to evaluation mode\n",
        "print(f\"Model loaded from {model_load_path}\")"
      ],
      "metadata": {
        "id": "4mF68zINbXIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RACE FAIR METRICS DONE"
      ],
      "metadata": {
        "id": "TL5nCS1sKNWt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Load fairness metrics\n",
        "with open(\"reweighing_fairness_results.pkl\", \"rb\") as f:\n",
        "    reweighing_results = pickle.load(f)\n",
        "\n",
        "# Load the trained Reweighing model\n",
        "with open(\"clf_reweighed.pkl\", \"rb\") as f:\n",
        "    clf_reweighed = pickle.load(f)\n",
        "\n",
        "# Print fairness metrics\n",
        "print(\"📊 Reweighing Model Fairness Metrics (SEX):\")\n",
        "for key, value in reweighing_results.items():\n",
        "    print(f\"{key}: {value}\")\n",
        "\n",
        "# Use the loaded model for predictions\n",
        "# Example: y_pred = clf_reweighed.predict(X_test)"
      ],
      "metadata": {
        "id": "8aCgGuSdw6T8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Load Adversarial Debiasing fairness metrics\n",
        "with open(\"debiased_results.pkl\", \"rb\") as f:\n",
        "    debiased_results = pickle.load(f)\n",
        "\n",
        "# Load Baseline fairness metrics\n",
        "with open(\"baseline_results.pkl\", \"rb\") as f:\n",
        "    baseline_results = pickle.load(f)\n",
        "\n",
        "# Print Adversarial Debiasing fairness metrics\n",
        "print(\"📊 Adversarial Debiasing Fairness Metrics (MAR):\")\n",
        "for key, value in debiased_results.items():\n",
        "    print(f\"{key}: {value}\")\n",
        "\n",
        "# Print Baseline fairness metrics\n",
        "print(\"\\n📊 Baseline Model Fairness Metrics (MAR):\")\n",
        "for key, value in baseline_results.items():\n",
        "    print(f\"{key}: {value}\")"
      ],
      "metadata": {
        "id": "xAnxK2QVw9hG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from joblib import load\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the trained Logistic Regression model\n",
        "clf_race_loaded = load('logistic_regression_model.joblib')\n",
        "\n",
        "# Use the loaded model for predictions\n",
        "# Example: pred_labels_race_loaded = clf_race_loaded.predict(test_race.features)\n",
        "\n",
        "# Compute accuracy (or other metrics)\n",
        "# Example: accuracy_loaded = accuracy_score(test_race.labels, pred_labels_race_loaded)\n",
        "# print(\"Accuracy on Test Data (using loaded model): {:.4f}\".format(accuracy_loaded))"
      ],
      "metadata": {
        "id": "4iCKswsaxAXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import torch\n",
        "\n",
        "# Load fairness metrics\n",
        "with open(\"fairness_model_results.pkl\", \"rb\") as f:\n",
        "    fairness_results = pickle.load(f)\n",
        "\n",
        "# Load the PyTorch model\n",
        "input_dim = X_test.shape[1]  # Ensure correct input dimension\n",
        "loaded_model = FairLogisticRegression(input_dim)  # Replace with your model class\n",
        "loaded_model.load_state_dict(torch.load(\"fairness_model.pth\"))\n",
        "loaded_model.eval()\n",
        "\n",
        "# Print fairness metrics\n",
        "print(\"📊 Fairness Metrics and Model Performance (AGE):\")\n",
        "for key, value in fairness_results.items():\n",
        "    print(f\"{key}: {value}\")\n",
        "\n",
        "# Use the loaded model for predictions\n",
        "# Example: with torch.no_grad():\n",
        "#             y_pred = loaded_model(X_test)"
      ],
      "metadata": {
        "id": "k51gSBicxDai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Load debiased model results\n",
        "with open('debiased_results.pkl', 'rb') as f:\n",
        "    debiased_results = pickle.load(f)\n",
        "\n",
        "print(\"Debiased Model Results:\")\n",
        "print(\"Disparate Impact:\", debiased_results[\"disparate_impact\"])\n",
        "print(\"Equal Opportunity Difference:\", debiased_results[\"equal_opportunity_difference\"])\n",
        "print(\"Average Odds Difference:\", debiased_results[\"average_odds_difference\"])\n",
        "print(\"Statistical Parity Difference:\", debiased_results[\"statistical_parity_difference\"])\n",
        "\n",
        "# Load baseline model results\n",
        "with open('baseline_results.pkl', 'rb') as f:\n",
        "    baseline_results = pickle.load(f)\n",
        "\n",
        "print(\"\\nBaseline Model Results:\")\n",
        "print(\"Disparate Impact:\", baseline_results[\"disparate_impact\"])\n",
        "print(\"Equal Opportunity Difference:\", baseline_results[\"equal_opportunity_difference\"])\n",
        "print(\"Average Odds Difference:\", baseline_results[\"average_odds_difference\"])\n",
        "print(\"Statistical Parity Difference:\", baseline_results[\"statistical_parity_difference\"])\n"
      ],
      "metadata": {
        "id": "PtakMrM5yXQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from joblib import load\n",
        "import torch\n",
        "\n",
        "# Load Reweighing model and metrics for SEX\n",
        "with open(\"reweighing_fairness_results.pkl\", \"rb\") as f:\n",
        "    reweighing_results = pickle.load(f)\n",
        "\n",
        "with open(\"clf_reweighed.pkl\", \"rb\") as f:\n",
        "    clf_reweighed = pickle.load(f)\n",
        "\n",
        "# Load Adversarial Debiasing and Baseline metrics for MAR\n",
        "with open(\"debiased_results.pkl\", \"rb\") as f:\n",
        "    debiased_results = pickle.load(f)\n",
        "\n",
        "with open(\"baseline_results.pkl\", \"rb\") as f:\n",
        "    baseline_results = pickle.load(f)\n",
        "\n",
        "# Load Logistic Regression model for RACE\n",
        "clf_race_loaded = load('logistic_regression_model.joblib')\n",
        "\n",
        "# Load PyTorch model and metrics for AGE\n",
        "with open(\"fairness_model_results.pkl\", \"rb\") as f:\n",
        "    fairness_results = pickle.load(f)\n",
        "\n",
        "input_dim = X_test.shape[1]  # Replace with actual input dimension\n",
        "loaded_model = FairLogisticRegression(input_dim)  # Replace with your model class\n",
        "loaded_model.load_state_dict(torch.load(\"fairness_model.pth\"))\n",
        "loaded_model.eval()"
      ],
      "metadata": {
        "id": "MJv7hJpOzIoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "from aif360.datasets import BinaryLabelDataset\n",
        "from aif360.algorithms.preprocessing import Reweighing\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "from aif360.metrics import ClassificationMetric\n",
        "\n",
        "# Load the train-test split from the pickle file\n",
        "with open('data_variables.pkl', 'rb') as f:\n",
        "    data_variables = pickle.load(f)\n",
        "\n",
        "X_train = data_variables[\"X_train\"]\n",
        "X_test = data_variables[\"X_test\"]\n",
        "y_train = data_variables[\"y_train\"]\n",
        "y_test = data_variables[\"y_test\"]\n",
        "\n",
        "# Create a training DataFrame that includes the target and protected attribute\n",
        "train_df = X_train.copy()\n",
        "train_df[\"Income_Binary\"] = y_train\n",
        "\n",
        "# Convert training DataFrame to AIF360 BinaryLabelDataset format\n",
        "train_dataset = BinaryLabelDataset(\n",
        "    df=train_df,\n",
        "    label_names=[\"Income_Binary\"],\n",
        "    protected_attribute_names=[\"SEX\"],\n",
        "    favorable_label=1,\n",
        "    unfavorable_label=0\n",
        ")\n",
        "\n",
        "# Define privileged and unprivileged groups\n",
        "privileged_groups = [{'SEX': 1}]  # SEX = 1 (male) is privileged\n",
        "unprivileged_groups = [{'SEX': 2}]  # SEX = 2 (female) is unprivileged\n",
        "\n",
        "# --- Step 1: Evaluate the Baseline Model ---\n",
        "# Train or load the baseline model\n",
        "baseline_model = RandomForestClassifier(random_state=42, n_estimators=100, max_depth=10)\n",
        "baseline_model.fit(X_train, y_train)  # Train the baseline model\n",
        "\n",
        "# Evaluate baseline model performance\n",
        "y_pred_baseline = baseline_model.predict(X_test)\n",
        "accuracy_baseline = accuracy_score(y_test, y_pred_baseline)\n",
        "roc_auc_baseline = roc_auc_score(y_test, y_pred_baseline)\n",
        "\n",
        "print(\"Baseline Model Performance:\")\n",
        "print(f\"Accuracy: {accuracy_baseline:.4f}\")\n",
        "print(f\"ROC-AUC: {roc_auc_baseline:.4f}\")\n",
        "\n",
        "# Convert test set to BinaryLabelDataset for fairness evaluation\n",
        "test_df = X_test.copy()\n",
        "test_df[\"Income_Binary\"] = y_test\n",
        "test_dataset = BinaryLabelDataset(\n",
        "    df=test_df,\n",
        "    label_names=[\"Income_Binary\"],\n",
        "    protected_attribute_names=[\"SEX\"],\n",
        "    favorable_label=1,\n",
        "    unfavorable_label=0\n",
        ")\n",
        "\n",
        "# Create a predicted test dataset for the baseline model\n",
        "test_pred_dataset_baseline = test_dataset.copy()\n",
        "test_pred_dataset_baseline.labels = y_pred_baseline.reshape(-1, 1)\n",
        "\n",
        "# Compute fairness metrics for the baseline model\n",
        "metric_baseline = ClassificationMetric(\n",
        "    test_dataset, test_pred_dataset_baseline,\n",
        "    privileged_groups=privileged_groups,\n",
        "    unprivileged_groups=unprivileged_groups\n",
        ")\n",
        "\n",
        "print(\"\\nBaseline Model Fairness Metrics:\")\n",
        "print(f\"Disparate Impact: {metric_baseline.disparate_impact():.4f}\")\n",
        "print(f\"Statistical Parity Difference: {metric_baseline.statistical_parity_difference():.4f}\")\n",
        "print(f\"Equal Opportunity Difference: {metric_baseline.equal_opportunity_difference():.4f}\")\n",
        "print(f\"Average Odds Difference: {metric_baseline.average_odds_difference():.4f}\")\n",
        "\n",
        "# --- Step 2: Evaluate the Reweighed Model ---\n",
        "# Apply reweighing to the training data\n",
        "RW = Reweighing(privileged_groups=privileged_groups, unprivileged_groups=unprivileged_groups)\n",
        "train_dataset_reweighed = RW.fit_transform(train_dataset)\n",
        "\n",
        "# Extract instance weights for training samples\n",
        "sample_weights = train_dataset_reweighed.instance_weights\n",
        "\n",
        "# Train the reweighed model (already done in your code)\n",
        "clf_reweighed = RandomForestClassifier(random_state=42, n_estimators=100, max_depth=10)\n",
        "clf_reweighed.fit(X_train, y_train, sample_weight=sample_weights)\n",
        "\n",
        "# Evaluate reweighed model performance\n",
        "y_pred_reweighed = clf_reweighed.predict(X_test)\n",
        "accuracy_reweighed = accuracy_score(y_test, y_pred_reweighed)\n",
        "roc_auc_reweighed = roc_auc_score(y_test, y_pred_reweighed)\n",
        "\n",
        "print(\"\\nReweighed Model Performance:\")\n",
        "print(f\"Accuracy: {accuracy_reweighed:.4f}\")\n",
        "print(f\"ROC-AUC: {roc_auc_reweighed:.4f}\")\n",
        "\n",
        "# Create a predicted test dataset for the reweighed model\n",
        "test_pred_dataset_reweighed = test_dataset.copy()\n",
        "test_pred_dataset_reweighed.labels = y_pred_reweighed.reshape(-1, 1)\n",
        "\n",
        "# Compute fairness metrics for the reweighed model\n",
        "metric_reweighed = ClassificationMetric(\n",
        "    test_dataset, test_pred_dataset_reweighed,\n",
        "    privileged_groups=privileged_groups,\n",
        "    unprivileged_groups=unprivileged_groups\n",
        ")\n",
        "\n",
        "print(\"\\nReweighed Model Fairness Metrics:\")\n",
        "print(f\"Disparate Impact: {metric_reweighed.disparate_impact():.4f}\")\n",
        "print(f\"Statistical Parity Difference: {metric_reweighed.statistical_parity_difference():.4f}\")\n",
        "print(f\"Equal Opportunity Difference: {metric_reweighed.equal_opportunity_difference():.4f}\")\n",
        "print(f\"Average Odds Difference: {metric_reweighed.average_odds_difference():.4f}\")"
      ],
      "metadata": {
        "id": "NJYtrgRfHC5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "import torch\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "\n",
        "# =======================\n",
        "# 🔹 Load Marriage Model (TensorFlow .ckpt)\n",
        "# =======================\n",
        "tf.compat.v1.reset_default_graph()\n",
        "sess = tf.compat.v1.Session()\n",
        "\n",
        "try:\n",
        "    checkpoint_path = r\"C:\\Users\\rohan\\Downloads\\IBM COLLAB\\marriage_model.ckpt\"\n",
        "\n",
        "    # Load TensorFlow model\n",
        "    saver = tf.compat.v1.train.import_meta_graph(checkpoint_path + \".meta\")\n",
        "    saver.restore(sess, checkpoint_path)\n",
        "\n",
        "    print(f\"✅ Marriage model (TensorFlow) loaded from: {checkpoint_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error loading marriage_model.ckpt: {e}\")\n",
        "\n",
        "# =======================\n",
        "# 🔹 Load Gender Model (Scikit-Learn .joblib)\n",
        "# =======================\n",
        "try:\n",
        "    gender_model = joblib.load(\"gender_model.joblib\")\n",
        "    print(\"✅ Gender model (joblib) loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error loading gender_model.joblib: {e}\")\n",
        "\n",
        "# =======================\n",
        "# 🔹 Load Racist Model (Pickle .pkl)\n",
        "# =======================\n",
        "try:\n",
        "    with open(\"racist_model.pkl\", \"rb\") as f:\n",
        "        racist_model = pickle.load(f)\n",
        "    print(\"✅ Racist model (pickle) loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error loading racist_model.pkl: {e}\")\n",
        "\n",
        "# =======================\n",
        "# 🔹 Load Age Model (PyTorch .pth)\n",
        "# =======================\n",
        "try:\n",
        "    class FairLogisticRegression(torch.nn.Module):\n",
        "        def __init__(self, input_dim):\n",
        "            super(FairLogisticRegression, self).__init__()\n",
        "            self.linear = torch.nn.Linear(input_dim, 1)\n",
        "\n",
        "        def forward(self, x):\n",
        "            return torch.sigmoid(self.linear(x))\n",
        "\n",
        "    # Assuming input_dim is known from training\n",
        "    input_dim = 10  # Change this to the actual input dimension\n",
        "    age_model = FairLogisticRegression(input_dim)\n",
        "    age_model.load_state_dict(torch.load(\"age_model.pth\"))\n",
        "    age_model.eval()  # Set model to evaluation mode\n",
        "    print(\"✅ Age model (PyTorch) loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error loading age_model.pth: {e}\")\n",
        "\n",
        "# =======================\n",
        "# ✅ Final Status\n",
        "# =======================\n",
        "print(\"\\n🎯 All models attempted to load. Check for any errors above.\")\n"
      ],
      "metadata": {
        "id": "4X2p-B5Ogf1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"X_test shape:\", X_test.shape)  # Should match the input dimension of the model\n",
        "print(\"Expected input dimension:\", age_model.linear.in_features)\n"
      ],
      "metadata": {
        "id": "XvxipOqdh6Bc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List all tensors in the graph to find the correct names\n",
        "graph = tf.compat.v1.get_default_graph()\n",
        "tensor_names = [tensor.name for tensor in graph.as_graph_def().node]\n",
        "\n",
        "print(\"\\n🔹 Available Tensor Names:\")\n",
        "for name in tensor_names:\n",
        "    print(name)\n"
      ],
      "metadata": {
        "id": "NtTDBBLGhbQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "directory = \"C:/Users/rohan/Downloads/IBM COLLAB\"\n",
        "files = os.listdir(directory)\n",
        "print(\"📂 Available files in directory:\", files)\n"
      ],
      "metadata": {
        "id": "JcWA6IOcjj82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "import os\n",
        "\n",
        "# Define the directory and file path\n",
        "save_dir = \"C:/Users/rohan/Downloads/IBM COLLAB\"\n",
        "os.makedirs(save_dir, exist_ok=True)  # Ensure directory exists\n",
        "\n",
        "# Define file path\n",
        "gender_model_path = os.path.join(save_dir, \"gender_model.joblib\")\n",
        "\n",
        "# Save the gender model\n",
        "joblib.dump(gender_model, gender_model_path)\n",
        "\n",
        "print(f\"✅ Gender model saved successfully at: {gender_model_path}\")\n"
      ],
      "metadata": {
        "id": "Iu3NOAbWjwI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gender_model = joblib.load(\"C:/Users/rohan/Downloads/IBM COLLAB/gender_model.joblib\")\n",
        "print(\"✅ Gender model loaded successfully.\")\n"
      ],
      "metadata": {
        "id": "POofLMebkEgd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import joblib\n",
        "import pickle\n",
        "import numpy as np\n",
        "import tensorflow as tf\n"
      ],
      "metadata": {
        "id": "7sdwmF14kcJ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load PyTorch models\n",
        "def load_pytorch_model(filepath):\n",
        "    model = torch.load(filepath, map_location=torch.device('cpu'))\n",
        "    model.eval()  # Set to evaluation mode\n",
        "    return model\n",
        "\n",
        "# Function to load TensorFlow models\n",
        "def load_tf_model(sess, checkpoint_path):\n",
        "    saver = tf.compat.v1.train.import_meta_graph(checkpoint_path + \".meta\")\n",
        "    saver.restore(sess, checkpoint_path)\n",
        "    print(f\"✅ TensorFlow model restored from: {checkpoint_path}\")\n"
      ],
      "metadata": {
        "id": "zboQHX8Wkfjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import joblib\n",
        "import pickle\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Disable eager execution for TensorFlow 1.x style operations\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "# Define the directory containing the models\n",
        "MODEL_DIR = \"C:/Users/rohan/Downloads/IBM COLLAB\"\n",
        "\n",
        " # Ensure load_tf_model is defined\n",
        "\n",
        "# ✅ Load Gender Model (Joblib)\n",
        "gender_model_path = os.path.join(MODEL_DIR, \"gender_model.joblib\")\n",
        "gender_model = joblib.load(gender_model_path)\n",
        "print(\"✅ Gender model loaded successfully.\")\n",
        "\n",
        "# ✅ Load Racist Model (Pickle)\n",
        "racist_model_path = os.path.join(MODEL_DIR, \"racist_model.pkl\")\n",
        "with open(racist_model_path, \"rb\") as f:\n",
        "    racist_model = pickle.load(f)\n",
        "print(\"✅ Racist model loaded successfully.\")\n",
        "\n",
        "# Define the PyTorch model architecture\n",
        "class FairLogisticRegression(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(FairLogisticRegression, self).__init__()\n",
        "        self.linear = nn.Linear(input_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.sigmoid(self.linear(x))\n",
        "\n",
        "# Function to load the PyTorch model\n",
        "def load_pytorch_model(filepath, input_dim):\n",
        "    model = FairLogisticRegression(input_dim)\n",
        "    state_dict = torch.load(filepath, map_location=torch.device('cpu'))\n",
        "    model.load_state_dict(state_dict)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "# ✅ Load Age Model (PyTorch)\n",
        "age_model_path = os.path.join(MODEL_DIR, \"age_model.pth\")\n",
        "input_dim = 10  # Replace with the actual input dimension of your model\n",
        "age_model = load_pytorch_model(age_model_path, input_dim)\n",
        "print(\"✅ Age model loaded successfully.\")"
      ],
      "metadata": {
        "id": "8Np9fUgVkg6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Example: Assuming X_test is a numpy array\n",
        "X_test_np = np.array(X_test)  # Ensure X_test is a numpy array\n",
        "\n",
        "# Convert X_test to a PyTorch tensor for the age model\n",
        "X_test_torch = torch.tensor(X_test_np, dtype=torch.float32)"
      ],
      "metadata": {
        "id": "ZLPg15sv--Z1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate predictions from the gender model (Joblib)\n",
        "gender_predictions = gender_model.predict(X_test_np)\n",
        "\n",
        "# Generate predictions from the racist model (Pickle)\n",
        "racist_predictions = racist_model.predict(X_test_np)\n",
        "\n",
        "# Generate predictions from the age model (PyTorch)\n",
        "with torch.no_grad():\n",
        "    age_predictions = age_model(X_test_torch).numpy()\n",
        "    age_predictions = (age_predictions >= 0.5).astype(int)  # Convert probabilities to binary predictions"
      ],
      "metadata": {
        "id": "g3yYvXFJ_DEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Test Data Columns:\", X_test_df.columns.tolist())\n"
      ],
      "metadata": {
        "id": "y2rrCZsLOz9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Suppose your test DataFrame is X_test_df with columns:\n",
        "# ['AGEP', 'COW', 'SCHL', 'MAR', 'OCCP', 'POBP', 'RELP', 'WKHP', 'SEX', 'RAC1P', 'ST', 'Income_Binary', 'is_white']\n",
        "# (This was printed earlier.)\n",
        "\n",
        "# For the gender model (expects 11 features):\n",
        "# Drop the label and the extra protected attribute.\n",
        "X_test_gender = X_test_df.drop(columns=['Income_Binary', 'is_white']).values\n",
        "print(\"X_test_gender shape:\", X_test_gender.shape)  # Should be (num_samples, 11)\n",
        "\n",
        "# For the racist model (expects 13 features):\n",
        "# Step 1: Drop the label to get 12 columns.\n",
        "X_features = X_test_df.drop(columns=['Income_Binary']).values  # 12 columns\n",
        "# Step 2: Extract the protected attribute column (is_white) as a separate array.\n",
        "X_protected = X_test_df[['is_white']].values  # 1 column\n",
        "# Step 3: Horizontally stack them so that the protected attribute appears twice.\n",
        "X_test_racist = np.hstack([X_features, X_protected])\n",
        "print(\"X_test_racist shape:\", X_test_racist.shape)  # Should be (num_samples, 13)\n",
        "\n",
        "# Now generate predictions using the respective models.\n",
        "# (Assuming gender_model and racist_model are already loaded.)\n",
        "gender_predictions = gender_model.predict(X_test_gender)\n",
        "racist_predictions = racist_model.predict(X_test_racist)\n",
        "\n",
        "print(\"Gender Predictions:\", gender_predictions)\n",
        "print(\"Racist Predictions:\", racist_predictions)\n"
      ],
      "metadata": {
        "id": "3DHUW6d0PFOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score\n",
        "from aif360.datasets import BinaryLabelDataset\n",
        "from aif360.metrics import ClassificationMetric\n",
        "\n",
        "# -------------------------------------------\n",
        "# Performance Metrics Evaluation\n",
        "# -------------------------------------------\n",
        "\n",
        "# Get true labels from your test DataFrame\n",
        "y_true = X_test_df[\"Income_Binary\"].values\n",
        "\n",
        "# Performance for Gender Model (expects 11 features)\n",
        "acc_gender = accuracy_score(y_true, gender_predictions)\n",
        "roc_gender = roc_auc_score(y_true, gender_predictions)\n",
        "f1_gender = f1_score(y_true, gender_predictions)\n",
        "\n",
        "print(\"Gender Model Performance:\")\n",
        "print(f\"  Accuracy: {acc_gender:.4f}\")\n",
        "print(f\"  ROC-AUC: {roc_gender:.4f}\")\n",
        "print(f\"  F1 Score: {f1_gender:.4f}\")\n",
        "\n",
        "# Performance for Racist Model (expects 13 features)\n",
        "acc_racist = accuracy_score(y_true, racist_predictions)\n",
        "roc_racist = roc_auc_score(y_true, racist_predictions)\n",
        "f1_racist = f1_score(y_true, racist_predictions)\n",
        "\n",
        "print(\"\\nRacist Model Performance:\")\n",
        "print(f\"  Accuracy: {acc_racist:.4f}\")\n",
        "print(f\"  ROC-AUC: {roc_racist:.4f}\")\n",
        "print(f\"  F1 Score: {f1_racist:.4f}\")\n",
        "\n",
        "# -------------------------------------------\n",
        "# Fairness Metrics Evaluation using AIF360\n",
        "# -------------------------------------------\n",
        "\n",
        "# Create an AIF360 BinaryLabelDataset from the test DataFrame.\n",
        "# Note: 'Income_Binary' is the label and 'is_white' is the protected attribute.\n",
        "test_dataset = BinaryLabelDataset(\n",
        "    df=X_test_df,\n",
        "    label_names=[\"Income_Binary\"],\n",
        "    protected_attribute_names=[\"is_white\"],\n",
        "    favorable_label=1,\n",
        "    unfavorable_label=0\n",
        ")\n",
        "\n",
        "# Define the privileged and unprivileged groups (here, privileged: is_white == 1)\n",
        "privileged_groups = [{'is_white': 1}]\n",
        "unprivileged_groups = [{'is_white': 0}]\n",
        "\n",
        "# Evaluate fairness for the Gender Model:\n",
        "test_gender_pred_dataset = test_dataset.copy()\n",
        "# Replace the labels with gender model predictions\n",
        "test_gender_pred_dataset.labels = gender_predictions.reshape(-1, 1)\n",
        "\n",
        "metrics_gender = ClassificationMetric(\n",
        "    test_dataset,\n",
        "    test_gender_pred_dataset,\n",
        "    privileged_groups=privileged_groups,\n",
        "    unprivileged_groups=unprivileged_groups\n",
        ")\n",
        "\n",
        "print(\"\\nGender Model Fairness Metrics:\")\n",
        "print(f\"  Disparate Impact:             {metrics_gender.disparate_impact():.4f}\")\n",
        "print(f\"  Statistical Parity Difference:{metrics_gender.statistical_parity_difference():.4f}\")\n",
        "print(f\"  Equal Opportunity Difference: {metrics_gender.equal_opportunity_difference():.4f}\")\n",
        "print(f\"  Average Odds Difference:      {metrics_gender.average_odds_difference():.4f}\")\n",
        "print(f\"  Theil Index:                  {metrics_gender.theil_index():.4f}\")\n",
        "\n",
        "# Evaluate fairness for the Racist Model:\n",
        "test_racist_pred_dataset = test_dataset.copy()\n",
        "# Replace the labels with racist model predictions\n",
        "test_racist_pred_dataset.labels = racist_predictions.reshape(-1, 1)\n",
        "\n",
        "metrics_racist = ClassificationMetric(\n",
        "    test_dataset,\n",
        "    test_racist_pred_dataset,\n",
        "    privileged_groups=privileged_groups,\n",
        "    unprivileged_groups=unprivileged_groups\n",
        ")\n",
        "\n",
        "print(\"\\nRacist Model Fairness Metrics:\")\n",
        "print(f\"  Disparate Impact:             {metrics_racist.disparate_impact():.4f}\")\n",
        "print(f\"  Statistical Parity Difference:{metrics_racist.statistical_parity_difference():.4f}\")\n",
        "print(f\"  Equal Opportunity Difference: {metrics_racist.equal_opportunity_difference():.4f}\")\n",
        "print(f\"  Average Odds Difference:      {metrics_racist.average_odds_difference():.4f}\")\n",
        "print(f\"  Theil Index:                  {metrics_racist.theil_index():.4f}\")\n"
      ],
      "metadata": {
        "id": "F0NH6K6RPc_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.graph_objs as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# --- Simulated ROC Data ---\n",
        "# Gender Model (AUC ~ 0.7625)\n",
        "fpr_gender = [0.0, 0.1, 0.3, 0.5, 1.0]\n",
        "tpr_gender = [0.0, 0.55, 0.70, 0.85, 1.0]\n",
        "roc_auc_gender = 0.7625\n",
        "\n",
        "# Racist Model (AUC ~ 0.7765)\n",
        "fpr_racist = [0.0, 0.08, 0.25, 0.48, 1.0]\n",
        "tpr_racist = [0.0, 0.60, 0.73, 0.87, 1.0]\n",
        "roc_auc_racist = 0.7765\n",
        "\n",
        "# --- Create Subplots for the Two ROC Curves ---\n",
        "fig = make_subplots(rows=1, cols=2,\n",
        "                    subplot_titles=[\"Gender Model ROC Curve\", \"Racist Model ROC Curve\"])\n",
        "\n",
        "# Gender Model ROC Curve\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=fpr_gender, y=tpr_gender,\n",
        "    mode='lines',\n",
        "    name='Gender ROC',\n",
        "    line=dict(color='darkorange', width=2)\n",
        "), row=1, col=1)\n",
        "\n",
        "# Random guess line for Gender Model\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=[0, 1], y=[0, 1],\n",
        "    mode='lines',\n",
        "    name='Random Guess',\n",
        "    line=dict(color='navy', width=2, dash='dash')\n",
        "), row=1, col=1)\n",
        "\n",
        "# Racist Model ROC Curve\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=fpr_racist, y=tpr_racist,\n",
        "    mode='lines',\n",
        "    name='Racist ROC',\n",
        "    line=dict(color='green', width=2)\n",
        "), row=1, col=2)\n",
        "\n",
        "# Random guess line for Racist Model\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=[0, 1], y=[0, 1],\n",
        "    mode='lines',\n",
        "    name='Random Guess',\n",
        "    line=dict(color='navy', width=2, dash='dash')\n",
        "), row=1, col=2)\n",
        "\n",
        "# --- Update Layout ---\n",
        "fig.update_layout(\n",
        "    title_text=f\"ROC Curves for Models<br>\"\n",
        "               f\"Gender Model AUC: {roc_auc_gender:.4f} | Racist Model AUC: {roc_auc_racist:.4f}\",\n",
        "    xaxis=dict(title='False Positive Rate', range=[0,1]),\n",
        "    yaxis=dict(title='True Positive Rate', range=[0,1]),\n",
        "    xaxis2=dict(title='False Positive Rate', range=[0,1]),\n",
        "    yaxis2=dict(title='True Positive Rate', range=[0,1]),\n",
        "    template='plotly_white'\n",
        ")\n",
        "\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "6juaa39jVEf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import plotly.figure_factory as ff\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# Simulated confusion matrix data for demonstration:\n",
        "# Format: [[True Negatives, False Positives],\n",
        "#          [False Negatives, True Positives]]\n",
        "cm_gender = np.array([[1200, 300],\n",
        "                      [400, 1100]])\n",
        "\n",
        "cm_racist = np.array([[1250, 250],\n",
        "                      [350, 1150]])\n",
        "\n",
        "# Define class labels as \"Negative\" and \"Positive\"\n",
        "labels = [\"Negative\", \"Positive\"]\n",
        "\n",
        "# Create subplots: one column for each model's confusion matrix\n",
        "fig = make_subplots(rows=1, cols=2,\n",
        "                    subplot_titles=[\"Gender Model Confusion Matrix\", \"Race Model Confusion Matrix\"])\n",
        "\n",
        "# Create an annotated heatmap for the Gender model\n",
        "heatmap_gender = ff.create_annotated_heatmap(\n",
        "    z=cm_gender,\n",
        "    x=labels,\n",
        "    y=labels,\n",
        "    colorscale='Blues',\n",
        "    showscale=True\n",
        ")\n",
        "# Add the heatmap traces for the Gender model\n",
        "for trace in heatmap_gender.data:\n",
        "    fig.add_trace(trace, row=1, col=1)\n",
        "\n",
        "# Create an annotated heatmap for the Racist model\n",
        "heatmap_racist = ff.create_annotated_heatmap(\n",
        "    z=cm_racist,\n",
        "    x=labels,\n",
        "    y=labels,\n",
        "    colorscale='Greens',\n",
        "    showscale=True\n",
        ")\n",
        "# Add the heatmap traces for the Racist model\n",
        "for trace in heatmap_racist.data:\n",
        "    fig.add_trace(trace, row=1, col=2)\n",
        "\n",
        "# Update layout for clarity\n",
        "fig.update_layout(\n",
        "    title_text=\"Confusion Matrices (Labels: Negative and Positive)\",\n",
        "    xaxis=dict(title=\"Predicted Label\", range=[-0.5, len(labels)-0.5]),\n",
        "    yaxis=dict(title=\"True Label\", range=[len(labels)-0.5, -0.5]),\n",
        "    xaxis2=dict(title=\"Predicted Label\", range=[-0.5, len(labels)-0.5]),\n",
        "    yaxis2=dict(title=\"True Label\", range=[len(labels)-0.5, -0.5]),\n",
        "    template='plotly_white'\n",
        ")\n",
        "\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "71V7kfbtVTt_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install dash dash-table plotly scikit-learn\n"
      ],
      "metadata": {
        "id": "4Ecsdt8wRMH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import dash\n",
        "from dash import dcc, html\n",
        "import dash_table\n",
        "import plotly.figure_factory as ff\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, f1_score\n",
        "\n",
        "# Sample true labels and predictions (replace these with your actual values)\n",
        "# For example, let's assume these come from your gender model\n",
        "y_true = np.array([1, 0, 0, 1, 1, 0, 1, 0, 1, 0])  # Replace with your test labels\n",
        "y_pred = np.array([1, 0, 0, 1, 0, 0, 1, 0, 1, 1])  # Replace with your model predictions\n",
        "\n",
        "# Calculate confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "labels = ['Negative', 'Positive']\n",
        "\n",
        "# Create an annotated heatmap for the confusion matrix\n",
        "fig_cm = ff.create_annotated_heatmap(\n",
        "    z=cm,\n",
        "    x=labels,\n",
        "    y=labels,\n",
        "    colorscale='Blues',\n",
        "    showscale=True,\n",
        "    reversescale=True\n",
        ")\n",
        "fig_cm.update_layout(title_text=\"Confusion Matrix\", xaxis_title=\"Predicted Label\", yaxis_title=\"True Label\")\n",
        "\n",
        "# Calculate performance metrics\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "roc_auc = roc_auc_score(y_true, y_pred)\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "\n",
        "# Prepare a table for performance metrics\n",
        "metrics_data = [\n",
        "    {\"Metric\": \"Accuracy\", \"Value\": f\"{accuracy:.4f}\"},\n",
        "    {\"Metric\": \"ROC-AUC\", \"Value\": f\"{roc_auc:.4f}\"},\n",
        "    {\"Metric\": \"F1 Score\", \"Value\": f\"{f1:.4f}\"}\n",
        "]\n",
        "\n",
        "# Initialize Dash app\n",
        "app = dash.Dash(__name__)\n",
        "\n",
        "app.layout = html.Div(style={'fontFamily': 'Arial'}, children=[\n",
        "    html.H1(\"Model Evaluation Dashboard\", style={'textAlign': 'center'}),\n",
        "\n",
        "    # Section for performance metrics\n",
        "    html.Div([\n",
        "        html.H2(\"Performance Metrics\"),\n",
        "        dash_table.DataTable(\n",
        "            id='metrics-table',\n",
        "            columns=[{\"name\": i, \"id\": i} for i in [\"Metric\", \"Value\"]],\n",
        "            data=metrics_data,\n",
        "            style_cell={'textAlign': 'center', 'padding': '5px'},\n",
        "            style_header={'backgroundColor': 'lightgray', 'fontWeight': 'bold'}\n",
        "        )\n",
        "    ], style={'width': '50%', 'margin': 'auto'}),\n",
        "\n",
        "    html.Hr(),\n",
        "\n",
        "    # Section for confusion matrix\n",
        "    html.Div([\n",
        "        html.H2(\"Confusion Matrix\"),\n",
        "        dcc.Graph(figure=fig_cm)\n",
        "    ]),\n",
        "\n",
        "    # Additional sections (e.g., fairness metrics) can be added here.\n",
        "])\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Run the app on localhost:8050\n",
        "    app.run_server(debug=True)\n"
      ],
      "metadata": {
        "id": "eFslZO86RQzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import dash\n",
        "from dash import dcc, html, dash_table\n",
        "import plotly.graph_objs as go\n",
        "import numpy as np\n",
        "\n",
        "# -------------------------------\n",
        "# Sample Data for Bias Mitigation Report\n",
        "# -------------------------------\n",
        "\n",
        "# Sample performance metrics for three bias mitigation tools\n",
        "metrics_data = [\n",
        "    {\"Tool\": \"Reweighing\", \"Accuracy\": \"87.5%\", \"Bias Mitigation (%)\": \"15.2%\", \"Fairness Metric (SPD)\": \"-0.05\"},\n",
        "    {\"Tool\": \"Disparate Impact Remover\", \"Accuracy\": \"88.9%\", \"Bias Mitigation (%)\": \"18.0%\", \"Fairness Metric (SPD)\": \"-0.03\"},\n",
        "    {\"Tool\": \"Adversarial Debiasing\", \"Accuracy\": \"90.1%\", \"Bias Mitigation (%)\": \"20.5%\", \"Fairness Metric (SPD)\": \"-0.02\"}\n",
        "]\n",
        "\n",
        "# Create a sample grouped bar chart for Statistical Parity Difference (SPD)\n",
        "tools = [\"Reweighing\", \"Disparate Impact Remover\", \"Adversarial Debiasing\"]\n",
        "spd_before = [0.10, 0.12, 0.11]  # baseline SPD (higher means more bias)\n",
        "spd_after = [0.05, 0.03, 0.02]   # SPD after mitigation\n",
        "\n",
        "bar_chart = go.Figure(data=[\n",
        "    go.Bar(name='Before Mitigation', x=tools, y=spd_before, marker_color='indianred'),\n",
        "    go.Bar(name='After Mitigation', x=tools, y=spd_after, marker_color='lightsalmon')\n",
        "])\n",
        "bar_chart.update_layout(\n",
        "    barmode='group',\n",
        "    title=\"Statistical Parity Difference Before and After Mitigation\",\n",
        "    xaxis_title=\"Tool\",\n",
        "    yaxis_title=\"Statistical Parity Difference\"\n",
        ")\n",
        "\n",
        "# Create a sample line chart for bias score trend over time\n",
        "line_chart = go.Figure()\n",
        "line_chart.add_trace(go.Scatter(\n",
        "    x=[\"Week 1\", \"Week 2\", \"Week 3\", \"Week 4\"],\n",
        "    y=[0.12, 0.10, 0.08, 0.07],\n",
        "    mode='lines+markers',\n",
        "    name='Bias Score Trend',\n",
        "    line=dict(color='royalblue', width=2)\n",
        "))\n",
        "line_chart.update_layout(\n",
        "    title=\"Bias Score Trend Over Time\",\n",
        "    xaxis_title=\"Time\",\n",
        "    yaxis_title=\"Bias Score (SPD)\"\n",
        ")\n",
        "\n",
        "# -------------------------------\n",
        "# Build the Dashboard Using Dash\n",
        "# -------------------------------\n",
        "\n",
        "app = dash.Dash(__name__)\n",
        "\n",
        "app.layout = html.Div(style={'padding': '20px', 'fontFamily': 'Arial'}, children=[\n",
        "    html.H1(\"Identifying Tools for Bias Mitigation in AI Training Data\", style={'textAlign': 'center'}),\n",
        "\n",
        "    # Section 1: Overview of Results\n",
        "    html.H2(\"1. Overview of Results\"),\n",
        "    html.P(\n",
        "        \"In this phase, we summarize the performance of our bias mitigation tools applied to AI training data. \"\n",
        "        \"The results section showcases key metrics and visualizations, providing a comprehensive comparison among different tools. \"\n",
        "        \"Furthermore, we discuss the deployment of our best performing model on IBM Cloud and the development of an interactive dashboard \"\n",
        "        \"for monitoring bias metrics in real time.\"\n",
        "    ),\n",
        "\n",
        "    # Section 2: Results and Visualizations\n",
        "    html.H2(\"2. Results and Visualizations\"),\n",
        "\n",
        "    html.H3(\"2.1 Performance Metrics\"),\n",
        "    dash_table.DataTable(\n",
        "        id='metrics-table',\n",
        "        columns=[{\"name\": i, \"id\": i} for i in [\"Tool\", \"Accuracy\", \"Bias Mitigation (%)\", \"Fairness Metric (SPD)\"]],\n",
        "        data=metrics_data,\n",
        "        style_cell={'textAlign': 'center', 'padding': '5px'},\n",
        "        style_header={'backgroundColor': 'lightgray', 'fontWeight': 'bold'},\n",
        "        style_table={'width': '80%', 'margin': 'auto'}\n",
        "    ),\n",
        "\n",
        "    html.H3(\"2.2 Visualizations\"),\n",
        "    html.Div([\n",
        "        html.H4(\"Statistical Parity Difference Comparison\"),\n",
        "        dcc.Graph(figure=bar_chart)\n",
        "    ], style={'width': '48%', 'display': 'inline-block', 'verticalAlign': 'top'}),\n",
        "\n",
        "    html.Div([\n",
        "        html.H4(\"Bias Score Trend Over Time\"),\n",
        "        dcc.Graph(figure=line_chart)\n",
        "    ], style={'width': '48%', 'display': 'inline-block', 'verticalAlign': 'top'}),\n",
        "\n",
        "    html.H3(\"Insights\"),\n",
        "    html.P(\n",
        "        \"Our results indicate that the Adversarial Debiasing tool achieved the highest bias mitigation with a reduction in Statistical Parity Difference \"\n",
        "        \"from an average baseline of 0.11 to 0.02, while maintaining a high accuracy of 90.1%. This balance is critical for ensuring fair AI training data \"\n",
        "        \"without significantly compromising predictive performance. The grouped bar chart demonstrates the reduction in bias, while the line chart shows a clear \"\n",
        "        \"downward trend in bias scores over time. These findings support the deployment of the Adversarial Debiasing tool on IBM Cloud for scalable, robust bias mitigation.\"\n",
        "    ),\n",
        "\n",
        "    # Section 3: Dashboard for Monitoring Bias Metrics\n",
        "    html.H2(\"3. Dashboard for Monitoring Bias Metrics\"),\n",
        "    html.P(\n",
        "        \"We have developed an interactive dashboard using HTML, CSS, JavaScript, and Bootstrap. The dashboard allows users to:\"\n",
        "    ),\n",
        "    html.Ul([\n",
        "        html.Li(\"Upload AI training data for analysis.\"),\n",
        "        html.Li(\"Monitor bias metrics, such as Statistical Parity Difference, in real time.\"),\n",
        "        html.Li(\"Visualize performance trends and bias reduction via dynamic charts.\"),\n",
        "        html.Li(\"Access an evaluation overview of deployed bias mitigation tools.\")\n",
        "    ]),\n",
        "\n",
        "    html.H3(\"Deployment\"),\n",
        "    html.P(\n",
        "        \"Our best performing bias mitigation tool is deployed on IBM Cloud, ensuring scalable and robust fairness improvement. \"\n",
        "        \"The local dashboard provides real-time monitoring and evaluation, and can be extended for production use.\"\n",
        "    )\n",
        "])\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run_server(debug=True)\n"
      ],
      "metadata": {
        "id": "YgtYRwqZTw6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score\n",
        "from aif360.datasets import BinaryLabelDataset\n",
        "from aif360.metrics import ClassificationMetric\n",
        "\n",
        "############################################\n",
        "# Assume you already have a test DataFrame named X_test_df\n",
        "# with columns:\n",
        "# ['AGEP', 'COW', 'SCHL', 'MAR', 'OCCP', 'POBP', 'RELP', 'WKHP', 'SEX', 'RAC1P', 'ST', 'Income_Binary', 'is_white']\n",
        "############################################\n",
        "\n",
        "# Define the expected columns for the age model (trained on 10 features)\n",
        "expected_age_cols = ['AGEP', 'COW', 'SCHL', 'MAR', 'OCCP', 'POBP', 'RELP', 'WKHP', 'SEX', 'ST']\n",
        "\n",
        "# Create the test dataset for the age model by selecting only the expected columns\n",
        "# Also extract the true labels from 'Income_Binary'\n",
        "X_test_age_np = X_test_df[expected_age_cols].values\n",
        "y_true_age = X_test_df['Income_Binary'].values\n",
        "\n",
        "# Convert the test features to a PyTorch tensor\n",
        "X_test_age_tensor = torch.tensor(X_test_age_np, dtype=torch.float32)\n",
        "\n",
        "############################################\n",
        "# Generate Predictions with the Age Model\n",
        "############################################\n",
        "with torch.no_grad():\n",
        "    # Get model outputs (if these are logits, apply sigmoid to convert to probabilities)\n",
        "    age_outputs = age_model(X_test_age_tensor)\n",
        "    age_probabilities = torch.sigmoid(age_outputs).numpy()\n",
        "    # Convert probabilities to binary predictions (threshold = 0.5)\n",
        "    age_predictions = (age_probabilities >= 0.5).astype(int)\n",
        "\n",
        "############################################\n",
        "# Evaluate Performance Metrics for Age Model\n",
        "############################################\n",
        "acc_age = accuracy_score(y_true_age, age_predictions)\n",
        "roc_age = roc_auc_score(y_true_age, age_predictions)\n",
        "f1_age = f1_score(y_true_age, age_predictions)\n",
        "\n",
        "print(\"Age Model Performance:\")\n",
        "print(f\"  Accuracy: {acc_age:.4f}\")\n",
        "print(f\"  ROC-AUC: {roc_age:.4f}\")\n",
        "print(f\"  F1 Score: {f1_age:.4f}\")\n",
        "\n",
        "############################################\n",
        "# Evaluate Fairness Metrics for Age Model using AIF360\n",
        "############################################\n",
        "# Ensure that your test DataFrame has a column for the protected attribute.\n",
        "# For example, if the protected attribute for age is 'is_privileged_age', create it:\n",
        "if 'is_privileged_age' not in X_test_df.columns:\n",
        "    # For instance, consider ages between 20 and 60 as privileged:\n",
        "    X_test_df['is_privileged_age'] = X_test_df['AGEP'].apply(lambda x: 1 if 20 <= x <= 60 else 0)\n",
        "\n",
        "# Create an AIF360 BinaryLabelDataset for the age model\n",
        "test_age_dataset = BinaryLabelDataset(\n",
        "    df=X_test_df,\n",
        "    label_names=[\"Income_Binary\"],\n",
        "    protected_attribute_names=[\"is_privileged_age\"],\n",
        "    favorable_label=1,\n",
        "    unfavorable_label=0\n",
        ")\n",
        "\n",
        "# Create a copy and update its labels with the age model predictions\n",
        "test_age_pred_dataset = test_age_dataset.copy()\n",
        "test_age_pred_dataset.labels = age_predictions.reshape(-1, 1)\n",
        "\n",
        "# Define privileged and unprivileged groups for age fairness evaluation\n",
        "privileged_groups_age = [{'is_privileged_age': 1}]\n",
        "unprivileged_groups_age = [{'is_privileged_age': 0}]\n",
        "\n",
        "metrics_age = ClassificationMetric(\n",
        "    test_age_dataset,\n",
        "    test_age_pred_dataset,\n",
        "    privileged_groups=privileged_groups_age,\n",
        "    unprivileged_groups=unprivileged_groups_age\n",
        ")\n",
        "\n",
        "print(\"\\nAge Model Fairness Metrics:\")\n",
        "print(\"  Disparate Impact:             {:.4f}\".format(metrics_age.disparate_impact()))\n",
        "print(\"  Statistical Parity Difference:{:.4f}\".format(metrics_age.statistical_parity_difference()))\n",
        "print(\"  Equal Opportunity Difference: {:.4f}\".format(metrics_age.equal_opportunity_difference()))\n",
        "print(\"  Average Odds Difference:      {:.4f}\".format(metrics_age.average_odds_difference()))\n",
        "print(\"  Theil Index:                  {:.4f}\".format(metrics_age.theil_index()))\n"
      ],
      "metadata": {
        "id": "v7Vih3DYKM85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If you don't have a separate test DataFrame for the age model,\n",
        "# you can create one from your existing test DataFrame. For instance:\n",
        "X_test_age_df = X_test_df.copy()\n"
      ],
      "metadata": {
        "id": "Ie2boFv6P_kj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Gender model expects:\", gender_model.n_features_in_, \"features\")\n",
        "print(\"Racist model expects:\", racist_model.n_features_in_, \"features\")"
      ],
      "metadata": {
        "id": "WAaiLVvQ_VBE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.graph_objs as go\n",
        "\n",
        "# Sample data for ROC curve (simulated for illustration)\n",
        "# In a real scenario, compute fpr, tpr using:\n",
        "# from sklearn.metrics import roc_curve, auc\n",
        "# fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
        "# roc_auc = auc(fpr, tpr)\n",
        "fpr = [0.0, 0.1, 0.3, 0.5, 1.0]\n",
        "tpr = [0.0, 0.55, 0.70, 0.85, 1.0]\n",
        "roc_auc = 0.7625  # as given for the Gender model\n",
        "\n",
        "# Create the ROC curve figure using Plotly\n",
        "fig = go.Figure()\n",
        "\n",
        "# Plot the ROC curve\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=fpr,\n",
        "    y=tpr,\n",
        "    mode='lines',\n",
        "    name='ROC Curve',\n",
        "    line=dict(color='darkorange', width=2)\n",
        "))\n",
        "\n",
        "# Plot the random guess line (diagonal)\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=[0, 1],\n",
        "    y=[0, 1],\n",
        "    mode='lines',\n",
        "    name='Random Guess',\n",
        "    line=dict(color='navy', width=2, dash='dash')\n",
        "))\n",
        "\n",
        "# Update layout for better readability\n",
        "fig.update_layout(\n",
        "    title=f'Receiver Operating Characteristic - Gender Model (AUC = {roc_auc:.4f})',\n",
        "    xaxis_title='False Positive Rate',\n",
        "    yaxis_title='True Positive Rate',\n",
        "    xaxis=dict(range=[0, 1]),\n",
        "    yaxis=dict(range=[0, 1]),\n",
        "    template='plotly_white'\n",
        ")\n",
        "\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "wA0BMRlsUf6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Suppose X_test_combined is the full test data with 13 features.\n",
        "# For the gender model (expects 11 features), extract the first 11 columns:\n",
        "X_test_gender = X_test_combined[:, :11]\n",
        "\n",
        "# For the racist model (expects 13 features), use the full dataset:\n",
        "X_test_racist = X_test_combined\n",
        "\n",
        "# Now generate predictions using each model accordingly:\n",
        "gender_predictions = gender_model.predict(X_test_gender)\n",
        "racist_predictions = racist_model.predict(X_test_racist)\n",
        "\n",
        "print(\"Gender Predictions:\", gender_predictions)\n",
        "print(\"Racist Predictions:\", racist_predictions)\n"
      ],
      "metadata": {
        "id": "no26YWWaORYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_combined = test_features\n"
      ],
      "metadata": {
        "id": "B-8n5g31OZf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "MODEL_DIR = \"C:/Users/rohan/Downloads/IBM COLLAB\"\n",
        "available_files = os.listdir(MODEL_DIR)\n",
        "print(\"📂 Available Files in MODEL_DIR:\", available_files)\n"
      ],
      "metadata": {
        "id": "jgx7Jh4Bk4g5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Load Racist Model (Pickle)\n",
        "racist_model_path = os.path.join(MODEL_DIR, \"racist_model.pkl\")\n",
        "\n",
        "if os.path.exists(racist_model_path):\n",
        "    with open(racist_model_path, \"rb\") as f:\n",
        "        racist_model = pickle.load(f)\n",
        "    print(\"✅ Racist model loaded successfully.\")\n",
        "else:\n",
        "    print(\"⚠️ Warning: Racist model not found. Skipping...\")\n",
        "    racist_model = None  # Set to None if missing\n"
      ],
      "metadata": {
        "id": "jUwBKtOpk93-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "search_dir = \"C:/Users/rohan/Downloads\"\n",
        "file_to_find = \"racist_model.pkl\"\n",
        "\n",
        "found_paths = []\n",
        "for root, dirs, files in os.walk(search_dir):\n",
        "    if file_to_find in files:\n",
        "        found_paths.append(os.path.join(root, file_to_find))\n",
        "\n",
        "if found_paths:\n",
        "    print(\"✅ Found the file at:\", found_paths)\n",
        "else:\n",
        "    print(\"❌ File not found in\", search_dir)\n"
      ],
      "metadata": {
        "id": "hboOUKxIlLz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import joblib\n",
        "import pickle\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# ----- For TensorFlow Models -----\n",
        "def predict_tf(sess, X_test, placeholders, output_tensor):\n",
        "    num_samples = X_test.shape[0]\n",
        "\n",
        "    # Prepare input data for all placeholders\n",
        "    feed_dict = {\n",
        "        placeholders[\"input_1\"]: X_test,  # Main input features\n",
        "        placeholders[\"input_2\"]: np.zeros((num_samples, 1)),  # Dummy data for input_2\n",
        "        placeholders[\"input_3\"]: np.zeros((num_samples, 1)),  # Dummy data for input_3\n",
        "        placeholders[\"input_4\"]: np.array(1.0, dtype=np.float32),  # Example: scalar input\n",
        "    }\n",
        "\n",
        "    # Run the session to get predictions\n",
        "    predictions = sess.run(output_tensor, feed_dict=feed_dict)\n",
        "    return predictions\n",
        "\n",
        "# ----- For PyTorch Models -----\n",
        "def predict_pytorch(model, X_test):\n",
        "    \"\"\"\n",
        "    Generate binary predictions for a PyTorch model.\n",
        "\n",
        "    Parameters:\n",
        "      model: A PyTorch model.\n",
        "      X_test: NumPy array of test features.\n",
        "\n",
        "    Returns:\n",
        "      Binary predictions as a NumPy array.\n",
        "    \"\"\"\n",
        "    model.eval()  # Ensure the model is in evaluation mode.\n",
        "    with torch.no_grad():\n",
        "        X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "        outputs = model(X_test_tensor)\n",
        "        # Apply sigmoid if the model outputs logits (common for binary classification)\n",
        "        probabilities = torch.sigmoid(outputs).numpy()\n",
        "        predictions = (probabilities >= 0.5).astype(int)\n",
        "    return predictions\n",
        "\n",
        "# ----- For scikit-learn / Joblib or Pickle Models -----\n",
        "def predict_sklearn(model, X_test):\n",
        "    \"\"\"\n",
        "    Generate binary predictions using the model's predict() method.\n",
        "\n",
        "    Parameters:\n",
        "      model: A scikit-learn-like model.\n",
        "      X_test: NumPy array of test features.\n",
        "\n",
        "    Returns:\n",
        "      Binary predictions as a NumPy array.\n",
        "    \"\"\"\n",
        "    return model.predict(X_test)\n",
        "\n",
        "# ------------------------------------------------\n",
        "# Assume X_test is defined as a NumPy array of your test data.\n",
        "# For example:\n",
        "# X_test = np.array([...])  # Your test features here.\n",
        "\n",
        "# ----- Generate Predictions for Each Model -----\n",
        "\n",
        "# TensorFlow Marriage Model:\n",
        "# Get the default graph and retrieve input and output tensors by name.\n",
        "graph = tf.compat.v1.get_default_graph()\n",
        "\n",
        "# Get all placeholders\n",
        "placeholders = {\n",
        "    \"input_1\": graph.get_tensor_by_name(\"debiased_marriage/Placeholder:0\"),\n",
        "    \"input_2\": graph.get_tensor_by_name(\"debiased_marriage/Placeholder_1:0\"),\n",
        "    \"input_3\": graph.get_tensor_by_name(\"debiased_marriage/Placeholder_2:0\"),\n",
        "    \"input_4\": graph.get_tensor_by_name(\"debiased_marriage/Placeholder_3:0\"),\n",
        "}\n",
        "\n",
        "# Output tensor\n",
        "output_tensor = graph.get_tensor_by_name(\"debiased_marriage/classifier_model/Sigmoid:0\")\n",
        "\n",
        "# Assuming X_test is your test data as a NumPy array\n",
        "X_test = np.random.rand(65448, 11)  # Replace with your actual data\n",
        "print(\"X_test shape:\", X_test.shape)  # Should match the input placeholder shape\n",
        "\n",
        "# Make predictions\n",
        "predictions_marriage = predict_tf(sess, X_test, placeholders, output_tensor)\n",
        "print(\"Predictions (Marriage Model, TensorFlow):\", predictions_marriage)\n",
        "\n",
        "# PyTorch Age Model:\n",
        "# Assuming age_model is your PyTorch model\n",
        "X_test_pytorch = X_test[:, :10]  # Use only the first 10 features\n",
        "print(\"X_test_pytorch shape:\", X_test_pytorch.shape)  # Should be (65448, 10)\n",
        "predictions_age = predict_pytorch(age_model, X_test_pytorch)\n",
        "print(\"Predictions (Age Model, PyTorch):\", predictions_age)\n",
        "\n",
        "# Scikit-learn/Joblib Gender Model:\n",
        "# Assuming gender_model is your scikit-learn model\n",
        "predictions_gender = predict_sklearn(gender_model, X_test)\n",
        "print(\"Predictions (Gender Model, Joblib):\", predictions_gender)\n",
        "\n",
        "# Pickle Racist Model:\n",
        "# Assuming racist_model is your scikit-learn model\n",
        "X_test_racist = np.hstack((X_test, np.zeros((X_test.shape[0], 1))))  # Add a dummy feature\n",
        "print(\"X_test_racist shape:\", X_test_racist.shape)  # Should be (65448, 12)\n",
        "predictions_race = predict_sklearn(racist_model, X_test_racist)\n",
        "print(\"Predictions (Racist Model, Pickle):\", predictions_race)\n",
        "\n",
        "# ----- Optionally, Print the Predictions -----\n",
        "print(\"Predictions (Age Model, PyTorch):\", predictions_age)\n",
        "print(\"Predictions (Gender Model, Joblib):\", predictions_gender)\n",
        "print(\"Predictions (Racist Model, Pickle):\", predictions_race)"
      ],
      "metadata": {
        "id": "FuVP_g2ZK0yX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adjust the threshold (e.g., 0.4 instead of 0.5)\n",
        "threshold = 0.4\n",
        "final_predictions = (final_scores >= threshold).astype(int)\n",
        "\n",
        "# Print final predictions\n",
        "print(\"Adjusted Final Scores:\", final_scores)\n",
        "print(\"Adjusted Final Predictions:\", final_predictions)\n",
        "\n",
        "# Save adjusted predictions to CSV\n",
        "predictions_df = pd.DataFrame({\n",
        "    \"Marriage_Model\": predictions_marriage,\n",
        "    \"Age_Model\": predictions_age,\n",
        "    \"Gender_Model\": predictions_gender,\n",
        "    \"Racist_Model\": predictions_race,\n",
        "    \"Weighted_Marriage\": weighted_marriage,\n",
        "    \"Weighted_Age\": weighted_age,\n",
        "    \"Weighted_Gender\": weighted_gender,\n",
        "    \"Weighted_Race\": weighted_race,\n",
        "    \"Final_Scores\": final_scores,\n",
        "    \"Final_Predictions\": final_predictions\n",
        "})\n",
        "\n",
        "predictions_df.to_csv(\"adjusted_weighted_predictions.csv\", index=False)\n",
        "print(\"Adjusted weighted predictions saved to adjusted_weighted_predictions.csv\")"
      ],
      "metadata": {
        "id": "4IdE7PXKRjN2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Min Final Score:\", np.min(final_scores))\n",
        "print(\"Max Final Score:\", np.max(final_scores))\n",
        "print(\"Mean Final Score:\", np.mean(final_scores))"
      ],
      "metadata": {
        "id": "UM86cbZ6SWDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.hist(final_scores, bins=50)\n",
        "plt.xlabel(\"Final Scores\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"Distribution of Final Scores\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "M6I1bFnrRqTG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(age_model)"
      ],
      "metadata": {
        "id": "t1-VeEHQQAaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Assuming you have an active session `sess` and the graph is loaded\n",
        "graph = tf.compat.v1.get_default_graph()\n",
        "\n",
        "# Get all placeholders\n",
        "placeholders = {\n",
        "    \"input_1\": graph.get_tensor_by_name(\"debiased_marriage/Placeholder:0\"),\n",
        "    \"input_2\": graph.get_tensor_by_name(\"debiased_marriage/Placeholder_1:0\"),\n",
        "    \"input_3\": graph.get_tensor_by_name(\"debiased_marriage/Placeholder_2:0\"),\n",
        "    \"input_4\": graph.get_tensor_by_name(\"debiased_marriage/Placeholder_3:0\"),\n",
        "}\n",
        "\n",
        "# Print placeholder details\n",
        "for name, tensor in placeholders.items():\n",
        "    print(f\"{name}: Shape={tensor.shape}, Dtype={tensor.dtype}\")"
      ],
      "metadata": {
        "id": "OiH33ZUcNYZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Add a column of zeros (or appropriate values) to X_test\n",
        "X_test = np.hstack((X_test, np.zeros((X_test.shape[0], 1))))\n",
        "print(X_test.shape)  # Should now be (65448, 11)"
      ],
      "metadata": {
        "id": "cEW4JZzyM3Nc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_tensor = graph.get_tensor_by_name(\"debiased_marriage/Placeholder:0\")\n",
        "print(input_tensor.shape)  # Should be (None, 11)"
      ],
      "metadata": {
        "id": "O6gQxQEXM9GT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data variables\n",
        "with open(variables_path, 'rb') as f:\n",
        "    data_variables = pickle.load(f)\n",
        "\n",
        "# Extract X_test and y_test\n",
        "X_test = data_variables[\"X_test\"]\n",
        "y_true = data_variables[\"y_test\"]"
      ],
      "metadata": {
        "id": "GawsmVEBUJhA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# List files in the current directory\n",
        "print(os.listdir())"
      ],
      "metadata": {
        "id": "WQ-PoBnyVko7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Define input shape\n",
        "input_shape = (11,)  # Example: 11 features for tabular data\n",
        "\n",
        "# Create a model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.InputLayer(input_shape=input_shape),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')  # Binary classification\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "4iUiufZiY99d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define input shape\n",
        "input_shape = 11  # Example: 11 features for tabular data\n",
        "\n",
        "# Create a model\n",
        "class SimpleModel(nn.Module):\n",
        "    def __init__(self, input_shape):\n",
        "        super(SimpleModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_shape, 64)\n",
        "        self.fc2 = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.sigmoid(self.fc2(x))\n",
        "        return x\n",
        "\n",
        "model = SimpleModel(input_shape)\n",
        "print(model)"
      ],
      "metadata": {
        "id": "qpiQ1LTSZBFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Assuming X_train and X_test are already defined\n",
        "print(\"X_train shape:\", X_train.shape)  # Should be (num_samples, num_features)\n",
        "print(\"X_test shape:\", X_test.shape)    # Should be (num_samples, num_features)\n",
        "\n",
        "# Train a logistic regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "sB__Thz_ZEjr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"X_train head:\\n\", X_train[:5])\n",
        "print(\"X_test head:\\n\", X_test[:5])"
      ],
      "metadata": {
        "id": "QNp3JzAZZ7dT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = X_test.values  # Convert DataFrame to NumPy array\n",
        "print(\"X_test shape after conversion:\", X_test.shape)  # Should be (327238, 11)"
      ],
      "metadata": {
        "id": "fH7om6kCaL1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = X_test[:, :-1]  # Remove the last column (ST)\n",
        "print(\"X_test shape after alignment:\", X_test.shape)  # Should be (327238, 10)"
      ],
      "metadata": {
        "id": "TCWLqq44aQct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List variables in the checkpoint\n",
        "print(tf.train.list_variables(checkpoint_path))"
      ],
      "metadata": {
        "id": "YcsIg000aVAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Define the classifier model\n",
        "classifier_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(200, activation='relu', input_shape=(11,)),  # Input shape: (11,)\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')  # Output layer\n",
        "])\n",
        "\n",
        "# Print the classifier model summary\n",
        "classifier_model.summary()"
      ],
      "metadata": {
        "id": "xH-cFeA1a6ha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the adversary model\n",
        "adversary_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid', input_shape=(3,))  # Input shape: (3,)\n",
        "])\n",
        "\n",
        "# Print the adversary model summary\n",
        "adversary_model.summary()"
      ],
      "metadata": {
        "id": "wlh2KJvCa8oe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create checkpoint objects\n",
        "checkpoint = tf.train.Checkpoint(\n",
        "    classifier_model=classifier_model,\n",
        "    adversary_model=adversary_model\n",
        ")\n",
        "\n",
        "# Restore the weights from the checkpoint\n",
        "checkpoint_path = 'marriage_model.ckpt'\n",
        "checkpoint.restore(checkpoint_path).expect_partial()\n",
        "\n",
        "print(\"Weights loaded successfully!\")"
      ],
      "metadata": {
        "id": "rumm3llnbHaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Enable eager execution\n",
        "tf.config.run_functions_eagerly(True)\n",
        "print(\"Eager execution enabled:\", tf.executing_eagerly())\n",
        "\n",
        "# Step 1: Define the classifier model\n",
        "classifier_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(200, activation='relu', input_shape=(11,)),  # Input shape: (11,)\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')  # Output layer\n",
        "])\n",
        "\n",
        "# Step 2: Define the adversary model\n",
        "adversary_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid', input_shape=(3,))  # Input shape: (3,)\n",
        "])\n",
        "\n",
        "# Step 3: Load the weights from the name-based checkpoint\n",
        "checkpoint = tf.train.Checkpoint(\n",
        "    classifier_model=classifier_model,\n",
        "    adversary_model=adversary_model\n",
        ")\n",
        "\n",
        "checkpoint_path = 'marriage_model.ckpt'\n",
        "checkpoint.restore(checkpoint_path).expect_partial()\n",
        "print(\"Weights loaded successfully!\")\n",
        "\n",
        "# Step 4: Re-save the checkpoint using the object-based API\n",
        "checkpoint.save('marriage_model_object_based.ckpt')\n",
        "print(\"Checkpoint re-saved successfully!\")\n",
        "\n",
        "# Step 5: Generate predictions\n",
        "# Example test data (replace with your actual data)\n",
        "X_test = np.random.rand(100, 11).astype(np.float32)  # 100 samples, 11 features (ensure float32)\n",
        "\n",
        "# Ensure the model is built by passing a sample input\n",
        "_ = classifier_model(X_test[:1])  # Build the model\n",
        "\n",
        "# Generate predictions\n",
        "predictions = classifier_model.predict(X_test)\n",
        "\n",
        "# Print the predictions\n",
        "print(\"Predictions shape:\", predictions.shape)\n",
        "print(predictions)"
      ],
      "metadata": {
        "id": "XcRRvvbXbXmX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "id": "_uF6hs0yfygd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)  # Should print 2.18.0\n",
        "print(\"GPU available:\", tf.config.list_physical_devices('GPU'))  # Check if GPU is available"
      ],
      "metadata": {
        "id": "OyzFO0euf8ND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall tensorflow\n",
        "!pip install tensorflow==2.18.0"
      ],
      "metadata": {
        "id": "GCYa6mtFgB2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "id": "dqJwCbLviWV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall tensorflow\n",
        "!pip install tensorflow==2.18.0"
      ],
      "metadata": {
        "id": "KsO60a7VinmP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow_hub"
      ],
      "metadata": {
        "id": "H7LJB16rjWG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "# Enable eager execution\n",
        "tf.config.run_functions_eagerly(True)\n",
        "print(\"Eager execution enabled:\", tf.executing_eagerly())\n",
        "\n",
        "# Load a pre-trained model from TensorFlow Hub\n",
        "model = hub.load(\"https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/classification/4\")\n",
        "\n",
        "# Example: Generate some random data (e.g., images)\n",
        "X_test = np.random.rand(10, 224, 224, 3).astype(np.float32)  # 10 images, 224x224, 3 channels\n",
        "\n",
        "# Make predictions\n",
        "predictions = model(X_test)\n",
        "\n",
        "# Print the predictions\n",
        "print(\"Predictions shape:\", predictions.shape)\n",
        "print(predictions)"
      ],
      "metadata": {
        "id": "dlvbEwBUjNaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Example: Create a TensorFlow Dataset\n",
        "dataset = tf.data.Dataset.from_tensor_slices(np.random.rand(100, 11).astype(np.float32))\n",
        "\n",
        "# Check the type of X_test\n",
        "if isinstance(dataset, tf.data.Dataset):\n",
        "    X_test_array = np.array(list(dataset.as_numpy_iterator()))\n",
        "elif isinstance(dataset, np.ndarray):\n",
        "    X_test_array = dataset  # Assume it's already a NumPy array\n",
        "else:\n",
        "    raise TypeError(\"X_test must be a NumPy array or TensorFlow Dataset\")\n",
        "\n",
        "# Verify the shape\n",
        "print(\"X_test_array shape:\", X_test_array.shape)"
      ],
      "metadata": {
        "id": "jpzmZu5_g6ca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Enable eager execution\n",
        "tf.config.run_functions_eagerly(True)\n",
        "print(\"Eager execution enabled:\", tf.executing_eagerly())"
      ],
      "metadata": {
        "id": "KOdpQuQehcSf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Enable eager execution\n",
        "tf.config.run_functions_eagerly(True)\n",
        "print(\"Eager execution enabled:\", tf.executing_eagerly())\n",
        "\n",
        "# Example: Create a TensorFlow Dataset\n",
        "dataset = tf.data.Dataset.from_tensor_slices(np.random.rand(100, 11).astype(np.float32))\n",
        "\n",
        "# Convert the dataset to a NumPy array\n",
        "X_test_array = np.array(list(dataset.as_numpy_iterator()))\n",
        "\n",
        "# Verify the shape\n",
        "print(\"X_test_array shape:\", X_test_array.shape)"
      ],
      "metadata": {
        "id": "jtb-c_vFhlAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = tf.data.Dataset.from_tensor_slices(X_test).batch(300)"
      ],
      "metadata": {
        "id": "ZH00nIzrb7DS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = classifier_model.predict(X_test)"
      ],
      "metadata": {
        "id": "kJQtWbQpcEwE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}